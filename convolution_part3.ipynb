{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of cuda_alexnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyoti246/alexnet_cuda_cudnn/blob/main/Copy_of_cuda_alexnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRVbBADSkiEX",
        "outputId": "34190501-d581-465c-e1a8-7e78e4be46bf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfWwRwXxtmmq",
        "outputId": "ab6b1379-d655-4b54-9012-c4a470019e43"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1pjesqxuuaN",
        "outputId": "c691c4b1-dc60-485a-8fa0-e828bf35ed8c"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-8c4e2njt\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-8c4e2njt\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp37-none-any.whl size=4307 sha256=5d849835507dc0a857182b48791de408621c77af84531f84fad08e5d0a807e80\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5entshwc/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8n98biok_XC",
        "outputId": "1fa2c76b-a569-4577-c3a6-b2fb319ffd7f"
      },
      "source": [
        "!git clone https://github.com/alessandrobessi/cuda-lab.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cuda-lab'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Total 56 (delta 0), reused 0 (delta 0), pack-reused 56\u001b[K\n",
            "Unpacking objects: 100% (56/56), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7aTgpgOlCBH"
      },
      "source": [
        "!chmod 755 cuda-lab/INSTALL.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PNyMQhIlH3Y",
        "outputId": "cce2489c-46d4-4a02-f45a-88798c9d74f5"
      },
      "source": [
        "!./cuda-lab/INSTALL.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "--2021-04-12 19:35:26--  https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 152.199.0.24\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|152.199.0.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://developer.nvidia.com/compute/cuda/8.0/prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb [following]\n",
            "--2021-04-12 19:35:26--  https://developer.nvidia.com/compute/cuda/8.0/prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb\n",
            "Reusing existing connection to developer.nvidia.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/8.0/secure/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb?ge8TEWN9FQ1slucppxHojvabrXFAtGw8ScwV8H-1hXVgT_iiPV9CsohryGciRtmt-1m0lDp8qcaY2Q0aU26TJwWjrvcFnFlh8M4aDjL2K_plSG8pPXZDKVH7uk_DD2lktGcJ-7pSr_EZbBhuCV4cHbI1XTn153aQfHERqmTL_Mi4dpIBJKunjgiqyRIviCN7BXjkXEl7DH123PN3QcO8Ho2hyw [following]\n",
            "--2021-04-12 19:35:27--  https://developer.download.nvidia.com/compute/cuda/8.0/secure/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb?ge8TEWN9FQ1slucppxHojvabrXFAtGw8ScwV8H-1hXVgT_iiPV9CsohryGciRtmt-1m0lDp8qcaY2Q0aU26TJwWjrvcFnFlh8M4aDjL2K_plSG8pPXZDKVH7uk_DD2lktGcJ-7pSr_EZbBhuCV4cHbI1XTn153aQfHERqmTL_Mi4dpIBJKunjgiqyRIviCN7BXjkXEl7DH123PN3QcO8Ho2hyw\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1913589814 (1.8G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb’\n",
            "\n",
            "cuda-repo-ubuntu160 100%[===================>]   1.78G  31.3MB/s    in 15s     \n",
            "\n",
            "2021-04-12 19:35:41 (123 MB/s) - ‘cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb’ saved [1913589814/1913589814]\n",
            "\n",
            "Selecting previously unselected package cuda-repo-ubuntu1604-8-0-local-ga2.\n",
            "(Reading database ... 160983 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb ...\n",
            "Unpacking cuda-repo-ubuntu1604-8-0-local-ga2 (8.0.61-1) ...\n",
            "Setting up cuda-repo-ubuntu1604-8-0-local-ga2 (8.0.61-1) ...\n",
            "Warning: The postinst maintainerscript of the package cuda-repo-ubuntu1604-8-0-local-ga2\n",
            "Warning: seems to use apt-key (provided by apt) without depending on gnupg or gnupg2.\n",
            "Warning: This will BREAK in the future and should be fixed by the package maintainer(s).\n",
            "Note: Check first if apt-key functionality is needed at all - it probably isn't!\n",
            "Warning: apt-key should not be used in scripts (called from postinst maintainerscript of the package cuda-repo-ubuntu1604-8-0-local-ga2)\n",
            "OK\n",
            "OK\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Note, selecting 'cuda-8-0' for regex 'cuda-8.0'\n",
            "Note, selecting 'libcuda-8.0-1' for regex 'cuda-8.0'\n",
            "The following additional packages will be installed:\n",
            "  cuda-command-line-tools-8-0 cuda-core-8-0 cuda-cublas-8-0\n",
            "  cuda-cublas-dev-8-0 cuda-cudart-8-0 cuda-cudart-dev-8-0 cuda-cufft-8-0\n",
            "  cuda-cufft-dev-8-0 cuda-curand-8-0 cuda-curand-dev-8-0 cuda-cusolver-8-0\n",
            "  cuda-cusolver-dev-8-0 cuda-cusparse-8-0 cuda-cusparse-dev-8-0\n",
            "  cuda-demo-suite-8-0 cuda-documentation-8-0 cuda-driver-dev-8-0\n",
            "  cuda-license-8-0 cuda-misc-headers-8-0 cuda-npp-8-0 cuda-npp-dev-8-0\n",
            "  cuda-nvgraph-8-0 cuda-nvgraph-dev-8-0 cuda-nvml-dev-8-0 cuda-nvrtc-8-0\n",
            "  cuda-nvrtc-dev-8-0 cuda-runtime-8-0 cuda-samples-8-0 cuda-toolkit-8-0\n",
            "  cuda-visual-tools-8-0\n",
            "The following NEW packages will be installed:\n",
            "  cuda-8-0 cuda-command-line-tools-8-0 cuda-core-8-0 cuda-cublas-8-0\n",
            "  cuda-cublas-dev-8-0 cuda-cudart-8-0 cuda-cudart-dev-8-0 cuda-cufft-8-0\n",
            "  cuda-cufft-dev-8-0 cuda-curand-8-0 cuda-curand-dev-8-0 cuda-cusolver-8-0\n",
            "  cuda-cusolver-dev-8-0 cuda-cusparse-8-0 cuda-cusparse-dev-8-0\n",
            "  cuda-demo-suite-8-0 cuda-documentation-8-0 cuda-driver-dev-8-0\n",
            "  cuda-license-8-0 cuda-misc-headers-8-0 cuda-npp-8-0 cuda-npp-dev-8-0\n",
            "  cuda-nvgraph-8-0 cuda-nvgraph-dev-8-0 cuda-nvml-dev-8-0 cuda-nvrtc-8-0\n",
            "  cuda-nvrtc-dev-8-0 cuda-runtime-8-0 cuda-samples-8-0 cuda-toolkit-8-0\n",
            "  cuda-visual-tools-8-0\n",
            "0 upgraded, 31 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 0 B/1,312 MB of archives.\n",
            "After this operation, 2,079 MB of additional disk space will be used.\n",
            "Get:1 file:/var/cuda-repo-8-0-local-ga2  cuda-license-8-0 8.0.61-1 [27.6 kB]\n",
            "Get:2 file:/var/cuda-repo-8-0-local-ga2  cuda-misc-headers-8-0 8.0.61-1 [1,077 kB]\n",
            "Get:3 file:/var/cuda-repo-8-0-local-ga2  cuda-core-8-0 8.0.61-1 [20.0 MB]\n",
            "Get:4 file:/var/cuda-repo-8-0-local-ga2  cuda-cudart-8-0 8.0.61-1 [135 kB]\n",
            "Get:5 file:/var/cuda-repo-8-0-local-ga2  cuda-driver-dev-8-0 8.0.61-1 [14.1 kB]\n",
            "Get:6 file:/var/cuda-repo-8-0-local-ga2  cuda-cudart-dev-8-0 8.0.61-1 [1,071 kB]\n",
            "Get:7 file:/var/cuda-repo-8-0-local-ga2  cuda-command-line-tools-8-0 8.0.61-1 [26.1 MB]\n",
            "Get:8 file:/var/cuda-repo-8-0-local-ga2  cuda-nvrtc-8-0 8.0.61-1 [9,585 kB]\n",
            "Get:9 file:/var/cuda-repo-8-0-local-ga2  cuda-nvrtc-dev-8-0 8.0.61-1 [10.8 kB]\n",
            "Get:10 file:/var/cuda-repo-8-0-local-ga2  cuda-cusolver-8-0 8.0.61-1 [29.3 MB]\n",
            "Get:11 file:/var/cuda-repo-8-0-local-ga2  cuda-cusolver-dev-8-0 8.0.61-1 [6,816 kB]\n",
            "Get:12 file:/var/cuda-repo-8-0-local-ga2  cuda-cublas-8-0 8.0.61-1 [27.2 MB]\n",
            "Get:13 file:/var/cuda-repo-8-0-local-ga2  cuda-cublas-dev-8-0 8.0.61-1 [57.4 MB]\n",
            "Get:14 file:/var/cuda-repo-8-0-local-ga2  cuda-cufft-8-0 8.0.61-1 [117 MB]\n",
            "Get:15 file:/var/cuda-repo-8-0-local-ga2  cuda-cufft-dev-8-0 8.0.61-1 [94.8 MB]\n",
            "Get:16 file:/var/cuda-repo-8-0-local-ga2  cuda-curand-8-0 8.0.61-1 [43.7 MB]\n",
            "Get:17 file:/var/cuda-repo-8-0-local-ga2  cuda-curand-dev-8-0 8.0.61-1 [67.7 MB]\n",
            "Get:18 file:/var/cuda-repo-8-0-local-ga2  cuda-cusparse-8-0 8.0.61-1 [28.8 MB]\n",
            "Get:19 file:/var/cuda-repo-8-0-local-ga2  cuda-cusparse-dev-8-0 8.0.61-1 [29.6 MB]\n",
            "Get:20 file:/var/cuda-repo-8-0-local-ga2  cuda-npp-8-0 8.0.61-1 [157 MB]\n",
            "Get:21 file:/var/cuda-repo-8-0-local-ga2  cuda-npp-dev-8-0 8.0.61-1 [82.3 MB]\n",
            "Get:22 file:/var/cuda-repo-8-0-local-ga2  cuda-samples-8-0 8.0.61-1 [101 MB]\n",
            "Get:23 file:/var/cuda-repo-8-0-local-ga2  cuda-documentation-8-0 8.0.61-1 [113 MB]\n",
            "Get:24 file:/var/cuda-repo-8-0-local-ga2  cuda-nvml-dev-8-0 8.0.61-1 [48.4 kB]\n",
            "Get:25 file:/var/cuda-repo-8-0-local-ga2  cuda-nvgraph-8-0 8.0.61-1 [2,948 kB]\n",
            "Get:26 file:/var/cuda-repo-8-0-local-ga2  cuda-nvgraph-dev-8-0 8.0.61-1 [3,028 kB]\n",
            "Get:27 file:/var/cuda-repo-8-0-local-ga2  cuda-visual-tools-8-0 8.0.61-1 [286 MB]\n",
            "Get:28 file:/var/cuda-repo-8-0-local-ga2  cuda-toolkit-8-0 8.0.61-1 [2,892 B]\n",
            "Get:29 file:/var/cuda-repo-8-0-local-ga2  cuda-runtime-8-0 8.0.61-1 [2,574 B]\n",
            "Get:30 file:/var/cuda-repo-8-0-local-ga2  cuda-demo-suite-8-0 8.0.61-1 [4,988 kB]\n",
            "Get:31 file:/var/cuda-repo-8-0-local-ga2  cuda-8-0 8.0.61-1 [2,556 B]\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package cuda-license-8-0.\n",
            "(Reading database ... 161077 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cuda-license-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-license-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-misc-headers-8-0.\n",
            "Preparing to unpack .../01-cuda-misc-headers-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-misc-headers-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-core-8-0.\n",
            "Preparing to unpack .../02-cuda-core-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-core-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cudart-8-0.\n",
            "Preparing to unpack .../03-cuda-cudart-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-8-0.\n",
            "Preparing to unpack .../04-cuda-driver-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-8-0.\n",
            "Preparing to unpack .../05-cuda-cudart-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-8-0.\n",
            "Preparing to unpack .../06-cuda-command-line-tools-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-8-0.\n",
            "Preparing to unpack .../07-cuda-nvrtc-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-8-0.\n",
            "Preparing to unpack .../08-cuda-nvrtc-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-8-0.\n",
            "Preparing to unpack .../09-cuda-cusolver-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-dev-8-0.\n",
            "Preparing to unpack .../10-cuda-cusolver-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cublas-8-0.\n",
            "Preparing to unpack .../11-cuda-cublas-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cublas-dev-8-0.\n",
            "Preparing to unpack .../12-cuda-cublas-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cufft-8-0.\n",
            "Preparing to unpack .../13-cuda-cufft-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cufft-dev-8-0.\n",
            "Preparing to unpack .../14-cuda-cufft-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-curand-8-0.\n",
            "Preparing to unpack .../15-cuda-curand-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-curand-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-curand-dev-8-0.\n",
            "Preparing to unpack .../16-cuda-curand-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-curand-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-8-0.\n",
            "Preparing to unpack .../17-cuda-cusparse-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-dev-8-0.\n",
            "Preparing to unpack .../18-cuda-cusparse-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-npp-8-0.\n",
            "Preparing to unpack .../19-cuda-npp-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-npp-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-npp-dev-8-0.\n",
            "Preparing to unpack .../20-cuda-npp-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-npp-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-samples-8-0.\n",
            "Preparing to unpack .../21-cuda-samples-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-samples-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-documentation-8-0.\n",
            "Preparing to unpack .../22-cuda-documentation-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-8-0.\n",
            "Preparing to unpack .../23-cuda-nvml-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-8-0.\n",
            "Preparing to unpack .../24-cuda-nvgraph-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-dev-8-0.\n",
            "Preparing to unpack .../25-cuda-nvgraph-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-8-0.\n",
            "Preparing to unpack .../26-cuda-visual-tools-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-8-0.\n",
            "Preparing to unpack .../27-cuda-toolkit-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-runtime-8-0.\n",
            "Preparing to unpack .../28-cuda-runtime-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-8-0.\n",
            "Preparing to unpack .../29-cuda-demo-suite-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-8-0.\n",
            "Preparing to unpack .../30-cuda-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-license-8-0 (8.0.61-1) ...\n",
            "*** LICENSE AGREEMENT ***\n",
            "By using this software you agree to fully comply with the terms and \n",
            "conditions of the EULA (End User License Agreement). The EULA is located\n",
            "at /usr/local/cuda-8.0/doc/EULA.txt. The EULA can also be found at\n",
            "http://docs.nvidia.com/cuda/eula/index.html. If you do not agree to the\n",
            "terms and conditions of the EULA, do not use the software.\n",
            "\n",
            "Setting up cuda-nvgraph-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cufft-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-npp-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvgraph-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cudart-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-driver-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusolver-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvml-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cufft-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-misc-headers-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusparse-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvrtc-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvrtc-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-curand-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cublas-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusolver-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-core-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-curand-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-npp-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cudart-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cublas-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-runtime-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusparse-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-command-line-tools-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-demo-suite-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-samples-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-visual-tools-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-documentation-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-toolkit-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-8-0 (8.0.61-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package gcc-5-base:amd64.\n",
            "(Reading database ... 170075 files and directories currently installed.)\n",
            "Preparing to unpack .../00-gcc-5-base_5.5.0-12ubuntu1_amd64.deb ...\n",
            "Unpacking gcc-5-base:amd64 (5.5.0-12ubuntu1) ...\n",
            "Selecting previously unselected package libisl15:amd64.\n",
            "Preparing to unpack .../01-libisl15_0.18-4_amd64.deb ...\n",
            "Unpacking libisl15:amd64 (0.18-4) ...\n",
            "Selecting previously unselected package cpp-5.\n",
            "Preparing to unpack .../02-cpp-5_5.5.0-12ubuntu1_amd64.deb ...\n",
            "Unpacking cpp-5 (5.5.0-12ubuntu1) ...\n",
            "Selecting previously unselected package cuda-cudart-11-2.\n",
            "Preparing to unpack .../03-cuda-cudart-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-11-2.\n",
            "Preparing to unpack .../04-cuda-nvrtc-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package libcublas-11-2.\n",
            "Preparing to unpack .../05-libcublas-11-2_11.4.1.1043-1_amd64.deb ...\n",
            "Unpacking libcublas-11-2 (11.4.1.1043-1) ...\n",
            "Selecting previously unselected package libcufft-11-2.\n",
            "Preparing to unpack .../06-libcufft-11-2_10.4.1.152-1_amd64.deb ...\n",
            "Unpacking libcufft-11-2 (10.4.1.152-1) ...\n",
            "Selecting previously unselected package libcurand-11-2.\n",
            "Preparing to unpack .../07-libcurand-11-2_10.2.3.152-1_amd64.deb ...\n",
            "Unpacking libcurand-11-2 (10.2.3.152-1) ...\n",
            "Selecting previously unselected package libcusolver-11-2.\n",
            "Preparing to unpack .../08-libcusolver-11-2_11.1.0.152-1_amd64.deb ...\n",
            "Unpacking libcusolver-11-2 (11.1.0.152-1) ...\n",
            "Selecting previously unselected package libcusparse-11-2.\n",
            "Preparing to unpack .../09-libcusparse-11-2_11.4.1.1152-1_amd64.deb ...\n",
            "Unpacking libcusparse-11-2 (11.4.1.1152-1) ...\n",
            "Selecting previously unselected package libnpp-11-2.\n",
            "Preparing to unpack .../10-libnpp-11-2_11.3.2.152-1_amd64.deb ...\n",
            "Unpacking libnpp-11-2 (11.3.2.152-1) ...\n",
            "Selecting previously unselected package libnvjpeg-11-2.\n",
            "Preparing to unpack .../11-libnvjpeg-11-2_11.4.0.152-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-11-2 (11.4.0.152-1) ...\n",
            "Selecting previously unselected package cuda-libraries-11-2.\n",
            "Preparing to unpack .../12-cuda-libraries-11-2_11.2.2-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-11-2 (11.2.2-1) ...\n",
            "Selecting previously unselected package cuda-runtime-11-2.\n",
            "Preparing to unpack .../13-cuda-runtime-11-2_11.2.2-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-11-2 (11.2.2-1) ...\n",
            "Selecting previously unselected package cuda-cuobjdump-11-2.\n",
            "Preparing to unpack .../14-cuda-cuobjdump-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-cuobjdump-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-cuxxfilt-11-2.\n",
            "Preparing to unpack .../15-cuda-cuxxfilt-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-cuxxfilt-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-11-2.\n",
            "Preparing to unpack .../16-cuda-driver-dev-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-11-2.\n",
            "Preparing to unpack .../17-cuda-cudart-dev-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-nvcc-11-2.\n",
            "Preparing to unpack .../18-cuda-nvcc-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-nvcc-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-nvprune-11-2.\n",
            "Preparing to unpack .../19-cuda-nvprune-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-nvprune-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-compiler-11-2.\n",
            "Preparing to unpack .../20-cuda-compiler-11-2_11.2.2-1_amd64.deb ...\n",
            "Unpacking cuda-compiler-11-2 (11.2.2-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-11-2.\n",
            "Preparing to unpack .../21-cuda-nvrtc-dev-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package libcublas-dev-11-2.\n",
            "Preparing to unpack .../22-libcublas-dev-11-2_11.4.1.1043-1_amd64.deb ...\n",
            "Unpacking libcublas-dev-11-2 (11.4.1.1043-1) ...\n",
            "Selecting previously unselected package libcufft-dev-11-2.\n",
            "Preparing to unpack .../23-libcufft-dev-11-2_10.4.1.152-1_amd64.deb ...\n",
            "Unpacking libcufft-dev-11-2 (10.4.1.152-1) ...\n",
            "Selecting previously unselected package libcurand-dev-11-2.\n",
            "Preparing to unpack .../24-libcurand-dev-11-2_10.2.3.152-1_amd64.deb ...\n",
            "Unpacking libcurand-dev-11-2 (10.2.3.152-1) ...\n",
            "Selecting previously unselected package libcusolver-dev-11-2.\n",
            "Preparing to unpack .../25-libcusolver-dev-11-2_11.1.0.152-1_amd64.deb ...\n",
            "Unpacking libcusolver-dev-11-2 (11.1.0.152-1) ...\n",
            "Selecting previously unselected package libcusparse-dev-11-2.\n",
            "Preparing to unpack .../26-libcusparse-dev-11-2_11.4.1.1152-1_amd64.deb ...\n",
            "Unpacking libcusparse-dev-11-2 (11.4.1.1152-1) ...\n",
            "Selecting previously unselected package libnpp-dev-11-2.\n",
            "Preparing to unpack .../27-libnpp-dev-11-2_11.3.2.152-1_amd64.deb ...\n",
            "Unpacking libnpp-dev-11-2 (11.3.2.152-1) ...\n",
            "Selecting previously unselected package libnvjpeg-dev-11-2.\n",
            "Preparing to unpack .../28-libnvjpeg-dev-11-2_11.4.0.152-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-dev-11-2 (11.4.0.152-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-11-2.\n",
            "Preparing to unpack .../29-cuda-libraries-dev-11-2_11.2.2-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-11-2 (11.2.2-1) ...\n",
            "Selecting previously unselected package cuda-cupti-11-2.\n",
            "Preparing to unpack .../30-cuda-cupti-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-cupti-dev-11-2.\n",
            "Preparing to unpack .../31-cuda-cupti-dev-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-dev-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-nvdisasm-11-2.\n",
            "Preparing to unpack .../32-cuda-nvdisasm-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-nvdisasm-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-gdb-11-2.\n",
            "Preparing to unpack .../33-cuda-gdb-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-gdb-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-memcheck-11-2.\n",
            "Preparing to unpack .../34-cuda-memcheck-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-memcheck-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-nvprof-11-2.\n",
            "Preparing to unpack .../35-cuda-nvprof-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-nvprof-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-nvtx-11-2.\n",
            "Preparing to unpack .../36-cuda-nvtx-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-nvtx-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-sanitizer-11-2.\n",
            "Preparing to unpack .../37-cuda-sanitizer-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-sanitizer-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-11-2.\n",
            "Preparing to unpack .../38-cuda-command-line-tools-11-2_11.2.2-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-11-2 (11.2.2-1) ...\n",
            "Selecting previously unselected package cuda-nsight-compute-11-2.\n",
            "Preparing to unpack .../39-cuda-nsight-compute-11-2_11.2.2-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-compute-11-2 (11.2.2-1) ...\n",
            "Selecting previously unselected package cuda-nsight-systems-11-2.\n",
            "Preparing to unpack .../40-cuda-nsight-systems-11-2_11.2.2-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-systems-11-2 (11.2.2-1) ...\n",
            "Selecting previously unselected package cuda-nsight-11-2.\n",
            "Preparing to unpack .../41-cuda-nsight-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-11-2.\n",
            "Preparing to unpack .../42-cuda-nvml-dev-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-nvvp-11-2.\n",
            "Preparing to unpack .../43-cuda-nvvp-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-nvvp-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-11-2.\n",
            "Preparing to unpack .../44-cuda-visual-tools-11-2_11.2.2-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-11-2 (11.2.2-1) ...\n",
            "Selecting previously unselected package cuda-tools-11-2.\n",
            "Preparing to unpack .../45-cuda-tools-11-2_11.2.2-1_amd64.deb ...\n",
            "Unpacking cuda-tools-11-2 (11.2.2-1) ...\n",
            "Selecting previously unselected package cuda-samples-11-2.\n",
            "Preparing to unpack .../46-cuda-samples-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-samples-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-documentation-11-2.\n",
            "Preparing to unpack .../47-cuda-documentation-11-2_11.2.154-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-11-2 (11.2.154-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-11-2.\n",
            "Preparing to unpack .../48-cuda-toolkit-11-2_11.2.2-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-11-2 (11.2.2-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-11-2.\n",
            "Preparing to unpack .../49-cuda-demo-suite-11-2_11.2.152-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-11-2 (11.2.152-1) ...\n",
            "Selecting previously unselected package cuda-11-2.\n",
            "Preparing to unpack .../50-cuda-11-2_11.2.2-1_amd64.deb ...\n",
            "Unpacking cuda-11-2 (11.2.2-1) ...\n",
            "Selecting previously unselected package cuda.\n",
            "Preparing to unpack .../51-cuda_11.2.2-1_amd64.deb ...\n",
            "Unpacking cuda (11.2.2-1) ...\n",
            "Selecting previously unselected package libasan2:amd64.\n",
            "Preparing to unpack .../52-libasan2_5.5.0-12ubuntu1_amd64.deb ...\n",
            "Unpacking libasan2:amd64 (5.5.0-12ubuntu1) ...\n",
            "Selecting previously unselected package libmpx0:amd64.\n",
            "Preparing to unpack .../53-libmpx0_5.5.0-12ubuntu1_amd64.deb ...\n",
            "Unpacking libmpx0:amd64 (5.5.0-12ubuntu1) ...\n",
            "Selecting previously unselected package libgcc-5-dev:amd64.\n",
            "Preparing to unpack .../54-libgcc-5-dev_5.5.0-12ubuntu1_amd64.deb ...\n",
            "Unpacking libgcc-5-dev:amd64 (5.5.0-12ubuntu1) ...\n",
            "Selecting previously unselected package gcc-5.\n",
            "Preparing to unpack .../55-gcc-5_5.5.0-12ubuntu1_amd64.deb ...\n",
            "Unpacking gcc-5 (5.5.0-12ubuntu1) ...\n",
            "Selecting previously unselected package libstdc++-5-dev:amd64.\n",
            "Preparing to unpack .../56-libstdc++-5-dev_5.5.0-12ubuntu1_amd64.deb ...\n",
            "Unpacking libstdc++-5-dev:amd64 (5.5.0-12ubuntu1) ...\n",
            "Selecting previously unselected package g++-5.\n",
            "Preparing to unpack .../57-g++-5_5.5.0-12ubuntu1_amd64.deb ...\n",
            "Unpacking g++-5 (5.5.0-12ubuntu1) ...\n",
            "Setting up libcufft-11-2 (10.4.1.152-1) ...\n",
            "Setting up libcusparse-11-2 (11.4.1.1152-1) ...\n",
            "Setting up libnpp-11-2 (11.3.2.152-1) ...\n",
            "Setting up cuda-nvrtc-11-2 (11.2.152-1) ...\n",
            "Setting up libcurand-11-2 (10.2.3.152-1) ...\n",
            "Setting up cuda-nvvp-11-2 (11.2.152-1) ...\n",
            "Setting up cuda-nvtx-11-2 (11.2.152-1) ...\n",
            "Setting up cuda-nvml-dev-11-2 (11.2.152-1) ...\n",
            "Setting up libnvjpeg-11-2 (11.4.0.152-1) ...\n",
            "Setting up cuda-nsight-compute-11-2 (11.2.2-1) ...\n",
            "Setting up libcufft-dev-11-2 (10.4.1.152-1) ...\n",
            "Setting up libcusparse-dev-11-2 (11.4.1.1152-1) ...\n",
            "Setting up cuda-cuobjdump-11-2 (11.2.152-1) ...\n",
            "Setting up libcurand-dev-11-2 (10.2.3.152-1) ...\n",
            "Setting up libnpp-dev-11-2 (11.3.2.152-1) ...\n",
            "Setting up cuda-driver-dev-11-2 (11.2.152-1) ...\n",
            "Setting up cuda-cudart-11-2 (11.2.152-1) ...\n",
            "Setting up libisl15:amd64 (0.18-4) ...\n",
            "Setting up cuda-cuxxfilt-11-2 (11.2.152-1) ...\n",
            "Setting up libnvjpeg-dev-11-2 (11.4.0.152-1) ...\n",
            "Setting up cuda-memcheck-11-2 (11.2.152-1) ...\n",
            "Setting up cuda-cudart-dev-11-2 (11.2.152-1) ...\n",
            "Setting up cuda-nvprune-11-2 (11.2.152-1) ...\n",
            "Setting up cuda-nvrtc-dev-11-2 (11.2.152-1) ...\n",
            "Setting up cuda-nsight-11-2 (11.2.152-1) ...\n",
            "Setting up libcublas-11-2 (11.4.1.1043-1) ...\n",
            "Setting up cuda-sanitizer-11-2 (11.2.152-1) ...\n",
            "Setting up cuda-nvdisasm-11-2 (11.2.152-1) ...\n",
            "Setting up cuda-nsight-systems-11-2 (11.2.2-1) ...\n",
            "Setting up libcusolver-11-2 (11.1.0.152-1) ...\n",
            "Setting up cuda-nvprof-11-2 (11.2.152-1) ...\n",
            "Setting up gcc-5-base:amd64 (5.5.0-12ubuntu1) ...\n",
            "Setting up cuda-libraries-11-2 (11.2.2-1) ...\n",
            "Setting up libmpx0:amd64 (5.5.0-12ubuntu1) ...\n",
            "Setting up libcusolver-dev-11-2 (11.1.0.152-1) ...\n",
            "Setting up libcublas-dev-11-2 (11.4.1.1043-1) ...\n",
            "Setting up libasan2:amd64 (5.5.0-12ubuntu1) ...\n",
            "Setting up cuda-libraries-dev-11-2 (11.2.2-1) ...\n",
            "Setting up cuda-nvcc-11-2 (11.2.152-1) ...\n",
            "Setting up libgcc-5-dev:amd64 (5.5.0-12ubuntu1) ...\n",
            "Setting up cuda-gdb-11-2 (11.2.152-1) ...\n",
            "Setting up cuda-runtime-11-2 (11.2.2-1) ...\n",
            "Setting up cpp-5 (5.5.0-12ubuntu1) ...\n",
            "Setting up libstdc++-5-dev:amd64 (5.5.0-12ubuntu1) ...\n",
            "Setting up cuda-demo-suite-11-2 (11.2.152-1) ...\n",
            "Setting up cuda-samples-11-2 (11.2.152-1) ...\n",
            "Setting up cuda-compiler-11-2 (11.2.2-1) ...\n",
            "Setting up cuda-visual-tools-11-2 (11.2.2-1) ...\n",
            "Setting up cuda-documentation-11-2 (11.2.154-1) ...\n",
            "Setting up cuda-cupti-11-2 (11.2.152-1) ...\n",
            "Setting up gcc-5 (5.5.0-12ubuntu1) ...\n",
            "Setting up g++-5 (5.5.0-12ubuntu1) ...\n",
            "Setting up cuda-cupti-dev-11-2 (11.2.152-1) ...\n",
            "Setting up cuda-command-line-tools-11-2 (11.2.2-1) ...\n",
            "Setting up cuda-tools-11-2 (11.2.2-1) ...\n",
            "Setting up cuda-toolkit-11-2 (11.2.2-1) ...\n",
            "Setting alternatives\n",
            "update-alternatives: using /usr/local/cuda-11.2 to provide /usr/local/cuda (cuda) in auto mode\n",
            "update-alternatives: using /usr/local/cuda-11.2 to provide /usr/local/cuda-11 (cuda-11) in auto mode\n",
            "Setting up cuda-11-2 (11.2.2-1) ...\n",
            "Setting up cuda (11.2.2-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n",
            "gcc (Ubuntu 5.5.0-12ubuntu1) 5.5.0 20171010\n",
            "Copyright (C) 2015 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n",
            "g++ (Ubuntu 5.5.0-12ubuntu1) 5.5.0 20171010\n",
            "Copyright (C) 2015 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n",
            "import os\n",
            "os.environ['PATH'] += ':/usr/local/cuda/bin'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MNu2VfAlN40"
      },
      "source": [
        "import os\n",
        "os.environ['PATH'] += ':/usr/local/cuda/bin'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvRrUfHfuyKG",
        "outputId": "eb6e510f-7b17-464f-fcf5-41fd339299ee"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uBYg93wQm1oI",
        "outputId": "9c2ed558-8b6e-4857-cf97-f6f801ca9d4c"
      },
      "source": [
        "%%cuda --name alex.cu\n",
        "#include <cudnn.h>\n",
        "#include <cublas_v2.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cstdlib>\n",
        "#include <cassert>\n",
        "#include <cstdlib>\n",
        "#include <iostream>\n",
        "#include <string>\n",
        "#include <random>\n",
        "#include <cmath>\n",
        "#include <stdio.h>\n",
        "#include <bits/stdc++.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "#define BATCH_SIZE 16\n",
        "#define MAX_THREADS_PER_BLOCK 1024 // according to GTX 1050 Ti\n",
        "#define LAYERS 5\n",
        "#define COMP_TIME 3\n",
        "#define EXEC_TIME 1\n",
        "#define NUM_BATCHES 2\n",
        "\n",
        "int roundUp(int num, int den)\n",
        "{\n",
        "\n",
        "  return((num + den - 1 )/(den));\n",
        "\n",
        "}\n",
        "\n",
        "struct convDim_t{\n",
        "\n",
        "  int Height;\n",
        "  int Width;\n",
        "  int Channels;\n",
        "  int Batch;\n",
        "};\n",
        "\n",
        "struct kernelDim_t{\n",
        "\n",
        "  int kernelSize;\n",
        "  int kernelHeight;\n",
        "  int kernelWidth;\n",
        "  int strideHeight;\n",
        "  int strideWidth;\n",
        "  int padHeight;\n",
        "  int padWidth;\n",
        "  int dilationHeight;\n",
        "  int dilationWidth;\n",
        "};\n",
        "\n",
        "\n",
        "convDim_t setConvSpecs(int ht, int wd, int ch, int bt){\n",
        "\n",
        "  convDim_t temp;\n",
        "  temp.Height = ht;\n",
        "  temp.Width = wd;\n",
        "  temp.Channels = ch;\n",
        "  temp.Batch = bt;\n",
        "\n",
        "  return temp;\n",
        "}\n",
        "\n",
        "kernelDim_t setKernelSpecs(int size, int fheight, int fwidth, int sheight, int swidth, int pheight, int pwidth, int dheight, int dwidth){\n",
        "\n",
        "  kernelDim_t layerKernel;\n",
        "  layerKernel.kernelSize = size;\n",
        "  layerKernel.kernelHeight = fheight;\n",
        "  layerKernel.kernelWidth = fwidth;\n",
        "  layerKernel.strideHeight = sheight;\n",
        "  layerKernel.strideWidth = swidth;\n",
        "  layerKernel.padHeight = pheight;\n",
        "  layerKernel.padWidth = pwidth;\n",
        "  layerKernel.dilationHeight = dheight;\n",
        "  layerKernel.dilationWidth = dwidth;\n",
        "\n",
        "  return layerKernel;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "#define checkCUDNN(expression)                             \\\n",
        "{                                                          \\\n",
        "  cudnnStatus_t status = (expression);                     \\\n",
        "  if (status != CUDNN_STATUS_SUCCESS) {                    \\\n",
        "    std::cerr << \"Error on line \" << __LINE__ << \": \"      \\\n",
        "              << cudnnGetErrorString(status) << std::endl; \\\n",
        "    std::exit(EXIT_FAILURE);                               \\\n",
        "  }                                                        \\\n",
        "}\n",
        "\n",
        "\n",
        "float alpha = 1.0;\n",
        "float beta = 0.0;\n",
        "\n",
        "class ConvLayers{\n",
        "    \n",
        "public:\n",
        "    float *kernelTensor{nullptr};\t\t\n",
        "    int layerIndex;\n",
        "\t  size_t workspace_bytes{0};\n",
        "\n",
        "    convDim_t outDims;\n",
        "    convDim_t inDims;\n",
        "    kernelDim_t kernelDims;\n",
        "\t    \n",
        "\n",
        "    cudnnTensorDescriptor_t input_descriptor;\n",
        "    cudnnTensorDescriptor_t output_descriptor;\n",
        "    cudnnFilterDescriptor_t kernel_descriptor;\n",
        "    cudnnConvolutionDescriptor_t convolution_descriptor;\n",
        "    cudnnConvolutionFwdAlgo_t convolution_algorithm;\n",
        "\n",
        "    ConvLayers(){}\n",
        "\n",
        "    ConvLayers(int index, convDim_t inDim, kernelDim_t kdims, convDim_t outDims){\n",
        "\n",
        "      this->inDims = inDim;\n",
        "      this->kernelDims = kdims;\n",
        "      this->layerIndex = index;\n",
        "\t    this->outDims = outDims;\n",
        "    }\n",
        "\n",
        "    void set(int index, convDim_t inDim, kernelDim_t kdims, convDim_t outDims){\n",
        "\n",
        "      this->inDims = inDim;\n",
        "      this->kernelDims = kdims;\n",
        "      this->layerIndex = index;\n",
        "      this->outDims = outDims;\n",
        "    }\n",
        "\n",
        "    void buildConvLayer();\n",
        "\n",
        "    void fwdProp(cudaStream_t stream, cudnnHandle_t cudnn, float *inputTensor, float* &outputTensor, void* &d_workspace);\n",
        "\n",
        "};\n",
        "\n",
        "void ConvLayers::buildConvLayer(){\n",
        "\tcheckCUDNN(cudnnCreateTensorDescriptor(&input_descriptor));\n",
        "\tcheckCUDNN(cudnnSetTensor4dDescriptor(input_descriptor,\n",
        "\t\t\t\t\t\t\t\t\t\t/*format=*/CUDNN_TENSOR_NHWC,\n",
        "\t\t\t\t\t\t\t\t\t\t/*dataType=*/CUDNN_DATA_FLOAT,\n",
        "\t\t\t\t\t\t\t\t\t\t/*batch_size=*/inDims.Batch,\n",
        "\t\t\t\t\t\t\t\t\t\t/*channels=*/inDims.Channels,\n",
        "\t\t\t\t\t\t\t\t\t\t/*image_height=*/inDims.Height,\n",
        "\t\t\t\t\t\t\t\t\t\t/*image_width=*/inDims.Width));\n",
        "\n",
        "\n",
        "\n",
        "\tcheckCUDNN(cudnnCreateTensorDescriptor(&output_descriptor));\n",
        "\tcheckCUDNN(cudnnSetTensor4dDescriptor(output_descriptor,\n",
        "\t\t\t\t\t\t\t\t\t\t/*format=*/CUDNN_TENSOR_NHWC,\n",
        "\t\t\t\t\t\t\t\t\t\t/*dataType=*/CUDNN_DATA_FLOAT,\n",
        "\t\t\t\t\t\t\t\t\t\t/*batch_size=*/outDims.Batch,\n",
        "\t\t\t\t\t\t\t\t\t\t/*channels=*/outDims.Channels,\n",
        "\t\t\t\t\t\t\t\t\t\t/*image_height=*/outDims.Height,\n",
        "\t\t\t\t\t\t\t\t\t\t/*image_width=*/outDims.Width));   \n",
        "\t\t\t\t\t\t\t\t\t\t\n",
        "\t\t\t\t\t\t\t\t\t\t\n",
        "\n",
        "\tcheckCUDNN(cudnnCreateFilterDescriptor(&kernel_descriptor));\n",
        "\tcheckCUDNN(cudnnSetFilter4dDescriptor(kernel_descriptor,\n",
        "\t\t\t\t\t\t\t\t\t\t/*dataType=*/CUDNN_DATA_FLOAT,\n",
        "\t\t\t\t\t\t\t\t\t\t/*format=*/CUDNN_TENSOR_NCHW,\n",
        "\t\t\t\t\t\t\t\t\t\t/*out_channels=*/outDims.Channels,\n",
        "\t\t\t\t\t\t\t\t\t\t/*in_channels=*/inDims.Channels,\n",
        "\t\t\t\t\t\t\t\t\t\t/*kernel_height=*/kernelDims.kernelHeight,\n",
        "\t\t\t\t\t\t\t\t\t\t/*kernel_width=*/kernelDims.kernelWidth)); \n",
        "\t\t\t\t\t\t\t\t\t\t\n",
        "\t\t\t\t\t\t\t\t\t\t\n",
        "\n",
        "\tcheckCUDNN(cudnnCreateConvolutionDescriptor(&convolution_descriptor));\n",
        "\tcheckCUDNN(cudnnSetConvolution2dDescriptor(convolution_descriptor,\n",
        "\t\t\t\t\t\t\t\t\t\t/*pad_height=*/kernelDims.padHeight,\n",
        "\t\t\t\t\t\t\t\t\t\t/*pad_width=*/kernelDims.padWidth,\n",
        "\t\t\t\t\t\t\t\t\t\t/*vertical_stride=*/kernelDims.strideHeight,\n",
        "\t\t\t\t\t\t\t\t\t\t/*horizontal_stride=*/kernelDims.strideWidth,\n",
        "\t\t\t\t\t\t\t\t\t\t/*dilation_height=*/kernelDims.dilationHeight,\n",
        "\t\t\t\t\t\t\t\t\t\t/*dilation_width=*/kernelDims.dilationWidth,\n",
        "\t\t\t\t\t\t\t\t\t\t/*mode=*/CUDNN_CROSS_CORRELATION,\n",
        "\t\t\t\t\t\t\t\t\t\t/*computeType=*/CUDNN_DATA_FLOAT));\n",
        "\n",
        "\n",
        "\tfloat h_kernel[outDims.Channels][inDims.Channels][kernelDims.kernelHeight][kernelDims.kernelWidth];\n",
        "    for (int kernel = 0; kernel < outDims.Channels; ++kernel) {\n",
        "    for (int channel = 0; channel < inDims.Channels; ++channel) {\n",
        "        for (int row = 0; row < kernelDims.kernelHeight; ++row) {\n",
        "        for (int column = 0; column < kernelDims.kernelWidth; ++column) {\n",
        "            h_kernel[kernel][channel][row][column] = 0.5;\n",
        "        }\n",
        "        }\n",
        "    }\n",
        "    }\n",
        "\n",
        "    cudaMalloc(&kernelTensor, sizeof(h_kernel));\n",
        "    cudaMemcpy(kernelTensor, h_kernel, sizeof(h_kernel), cudaMemcpyHostToDevice); \n",
        "}\n",
        "\n",
        "\n",
        "void ConvLayers::fwdProp(cudaStream_t stream, cudnnHandle_t cudnn, float *inputTensor, float* &outputTensor, void* &d_workspace)\n",
        "{\n",
        "\tcheckCUDNN(cudnnGetConvolutionForwardAlgorithm(cudnn,\n",
        "                                            input_descriptor,\n",
        "                                            kernel_descriptor,\n",
        "                                            convolution_descriptor,\n",
        "                                            output_descriptor,\n",
        "                                            CUDNN_CONVOLUTION_FWD_PREFER_FASTEST,\n",
        "                                            /*memoryLimitInBytes=*/0,\n",
        "                                            &convolution_algorithm));\n",
        "                                            \n",
        "\n",
        "    checkCUDNN(cudnnGetConvolutionForwardWorkspaceSize(cudnn,\n",
        "                                                    input_descriptor,\n",
        "                                                    kernel_descriptor,\n",
        "                                                    convolution_descriptor,\n",
        "                                                    output_descriptor,\n",
        "                                                    convolution_algorithm,\n",
        "                                                    &workspace_bytes));                  \n",
        "                \n",
        "    cudaMalloc(&d_workspace, workspace_bytes);       \n",
        "\n",
        "\tint out_bytes = outDims.Batch*outDims.Channels*outDims.Height*outDims.Width*sizeof(float);\n",
        "    cudaMalloc(&outputTensor, out_bytes);\n",
        "    cudaMemsetAsync(outputTensor, 0, out_bytes, stream);\n",
        "\n",
        "\n",
        "    checkCUDNN(cudnnConvolutionForward(cudnn,\n",
        "                                   &alpha,\n",
        "                                   input_descriptor,\n",
        "                                   inputTensor,\n",
        "                                   kernel_descriptor,\n",
        "                                   kernelTensor,\n",
        "                                   convolution_descriptor,\n",
        "                                   convolution_algorithm,\n",
        "                                   d_workspace,\n",
        "                                   workspace_bytes,\n",
        "                                   &beta,\n",
        "                                   output_descriptor,\n",
        "                                   outputTensor));\n",
        "\n",
        "}\n",
        "\n",
        "ConvLayers convlayer1;\n",
        "ConvLayers convlayer2;\n",
        "ConvLayers convlayer3;\n",
        "ConvLayers convlayer4;\n",
        "ConvLayers convlayer5;\n",
        "\n",
        "\n",
        "float img[BATCH_SIZE][224][224][3];\n",
        "float output1[BATCH_SIZE][224][224][10];\n",
        "float output2[BATCH_SIZE][224][224][30];\n",
        "float output3[BATCH_SIZE][224][224][30];\n",
        "float output4[BATCH_SIZE][224][224][10];\n",
        "float output5[BATCH_SIZE][224][224][3];\n",
        "\n",
        "class Kernel{\n",
        "\n",
        "  public :\n",
        "    cudaStream_t stream;\n",
        "    cublasHandle_t cublas;\n",
        "    cudnnHandle_t cudnn;\n",
        "    int id;\n",
        "    int layer;\n",
        "    double tbRatio;\n",
        "    // float ** twoDInput;\n",
        "    // float ** twoDOutput;\n",
        "    double execTime;\n",
        "    double compTime;\n",
        "    \n",
        "\n",
        "    Kernel(){}\n",
        "    \n",
        "    Kernel(cudaStream_t stream,\n",
        "        cublasHandle_t cublas,\n",
        "        cudnnHandle_t cudnn,\n",
        "        int layerIndex,\n",
        "        double startTime,\n",
        "        double deadTime,\n",
        "        double tbratio,\n",
        "        int id){\n",
        "            this->stream = stream;\n",
        "            this->cublas = cublas;\n",
        "            this->cudnn = cudnn;\n",
        "            this->layer = layerIndex;\n",
        "            this->execTime = deadTime - startTime;\n",
        "            this->compTime = LAYERS*COMP_TIME;\n",
        "            this->tbRatio = tbratio;\n",
        "            this->id = id;\n",
        "        }\n",
        "    \n",
        "    Kernel(cudaStream_t stream,\n",
        "        cublasHandle_t cublas,\n",
        "        cudnnHandle_t cudnn,\n",
        "        int layerIndex,\n",
        "        float **twoDInput,\n",
        "        double startTime,\n",
        "        double deadTime,\n",
        "        double tbratio,\n",
        "        int id){\n",
        "            this->stream = stream;\n",
        "            this->cublas = cublas;\n",
        "            this->cudnn = cudnn;\n",
        "            this->layer = layerIndex;\n",
        "            // this->twoDInput = twoDInput;\n",
        "            this->execTime = deadTime - startTime;\n",
        "            this->compTime = LAYERS*COMP_TIME;\n",
        "            this->tbRatio = tbratio;\n",
        "            this->id =id;\n",
        "        }\n",
        "    \n",
        "    void update()\n",
        "    {\n",
        "        layer++;\n",
        "        compTime -= COMP_TIME;\n",
        "        // TODO update tbratio properly\n",
        "        if(layer >= 7){\n",
        "            tbRatio = 0.2;\n",
        "            // twoDInput = twoDOutput;\n",
        "        }\n",
        "        // else\n",
        "        // {\n",
        "        //     fourDInput = fourDOutput;\n",
        "        // }\n",
        "    }\n",
        "\n",
        "\n",
        "};\n",
        "\n",
        "void execute(vector<Kernel> C);\n",
        "\n",
        "\n",
        "typedef pair<int, Kernel> Node;\n",
        "\n",
        "struct cmp\n",
        "{\n",
        "  bool operator()(const Node &A,const Node &B) const\n",
        "  {\n",
        "    if(A.first != B.first)\n",
        "            return A.first < B.first;\n",
        "        else\n",
        "            return A.second.execTime - A.second.compTime < B.second.execTime - B.second.compTime;\n",
        "  }\n",
        "};\n",
        "\n",
        "typedef set<Node, cmp> SET;\n",
        "\n",
        "bool compareKernel(Kernel k1, Kernel k2){\n",
        "    return k1.compTime < k2.compTime;\n",
        "}\n",
        "\n",
        "int getSlackTime(int compTime, int execTime){\n",
        "    return execTime - compTime;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "vector<Kernel> G;\n",
        "\n",
        "SET enqueue(SET Queue, vector<Kernel> k){\n",
        "    SET newQueue;\n",
        "    for(SET::iterator it=Queue.begin();it!=Queue.end();it++){\n",
        "        Kernel temp_kernel = it->second;\n",
        "        temp_kernel.execTime -= EXEC_TIME;\n",
        "        newQueue.insert({getSlackTime(temp_kernel.compTime,temp_kernel.execTime),temp_kernel});\n",
        "    }\n",
        "    for(int i=0;i<k.size();i++){\n",
        "        if(k[i].layer == LAYERS+1)\n",
        "            continue;\n",
        "        k[i].execTime -= EXEC_TIME;\n",
        "        cout<<\"Enqueue ID: \"<<k[i].id<<\" \"<<k[i].layer<<endl;\n",
        "        newQueue.insert({getSlackTime(k[i].compTime, k[i].execTime), k[i]});\n",
        "    }\n",
        "    return newQueue;\n",
        "}\n",
        "\n",
        "\n",
        "SET dequeue(SET& Queue){\n",
        "    SET newQueue;\n",
        "    \n",
        "    cout<<\"Printing Queue\"<<endl;\n",
        "    cout<<Queue.size()<<endl;\n",
        "    for(SET:: iterator it=Queue.begin();it!=Queue.end();it++){\n",
        "        Kernel temp_kernel = it->second;\n",
        "        cout<<\"Dequeu ID: \"<<temp_kernel.id<<\" \"<<temp_kernel.layer<<endl;\n",
        "        temp_kernel.execTime -= COMP_TIME;\n",
        "        newQueue.insert({getSlackTime(temp_kernel.compTime,temp_kernel.execTime),temp_kernel});\n",
        "    }\n",
        "    Queue.clear();\n",
        "    vector<Kernel> C;\n",
        "    if(G.size()>0){\n",
        "        execute(G);\n",
        "        cout<<\"Execution size \"<<G.size()<<endl;\n",
        "    }\n",
        "        \n",
        "    for(int i=0;i<G.size();i++){\n",
        "        cout<<\"EXEC Layer: \"<<G[i].layer<<endl;\n",
        "        cout<<\"Priority:\"<<G[i].execTime - G[i].compTime<<endl;\n",
        "        if(G[i].layer != LAYERS){\n",
        "            G[i].update();\n",
        "        }\n",
        "    }\n",
        "    newQueue = enqueue(newQueue, G);\n",
        "    G.clear();\n",
        "    Kernel temp = (*(newQueue.begin())).second;\n",
        "    newQueue.erase(newQueue.begin());\n",
        "    G.push_back(temp);\n",
        "    double total = 0;\n",
        "    total += temp.tbRatio;\n",
        "    if(total < 1){\n",
        "        for(SET::iterator it = newQueue.begin();it!=newQueue.end();){\n",
        "            if((*it).second.tbRatio<1){\n",
        "                Kernel temp2 = (*it).second;\n",
        "                total += temp2.tbRatio;\n",
        "                it = newQueue.erase(it);\n",
        "                G.push_back(temp2);\n",
        "            }\n",
        "            else\n",
        "            {\n",
        "                it++;\n",
        "            }\n",
        "            cout<<\"Total TBratio \"<<total<<endl;\n",
        "            if(total > 1)\n",
        "                break;\n",
        "        }\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        cout<<\"Single kernel execution\"<<endl;\n",
        "        execute(G);\n",
        "        for(int i=0;i<G.size();i++){\n",
        "            cout<<\"Priority:\"<<G[i].execTime - G[i].compTime<<endl;\n",
        "            if(G[i].layer != LAYERS){\n",
        "                G[i].update();\n",
        "            }\n",
        "            else\n",
        "                G[i].layer++;\n",
        "        }\n",
        "        newQueue = enqueue(newQueue, G);\n",
        "        G.clear();\n",
        "\n",
        "        return newQueue;\n",
        "    }\n",
        "\n",
        "    execute(G);\n",
        "    cout<<\"Execution size: \"<<G.size()<<endl;\n",
        "    for(int i=0;i<G.size();i++){\n",
        "        cout<<\"Priority:\"<<G[i].execTime - G[i].compTime<<endl;\n",
        "        if(G[i].layer != LAYERS){\n",
        "            G[i].update();\n",
        "        }\n",
        "        else\n",
        "        {\n",
        "            G[i].layer++;\n",
        "        }\n",
        "    }\n",
        "    newQueue = enqueue(newQueue, G);\n",
        "    G.clear();\n",
        "    return newQueue;   \n",
        "}\n",
        "\n",
        "void call_conv1(ConvLayers &convlayer1, cudaStream_t stream, cudnnHandle_t CUDNN, float input[][224][224][3], float output[BATCH_SIZE][224][224][10])\n",
        "{\n",
        "    float *inputTensor;\n",
        "\tfloat *outputTensor;\n",
        "\tint inp_size = convlayer1.inDims.Height * convlayer1.inDims.Width * convlayer1.inDims.Channels * convlayer1.inDims.Batch * sizeof(float);\n",
        "    cudaMalloc(&inputTensor, inp_size);\n",
        "    cudaMemcpyAsync(inputTensor, input, inp_size, cudaMemcpyHostToDevice, stream);\n",
        "\n",
        "    void *d_workspace;\n",
        "    convlayer1.fwdProp(stream, CUDNN, inputTensor, outputTensor, d_workspace);\n",
        "\n",
        "    int out_size =  convlayer1.outDims.Height*convlayer1.outDims.Width*convlayer1.outDims.Channels*convlayer1.outDims.Batch*sizeof(float);\n",
        "    \n",
        "    cudaMemcpyAsync(output, outputTensor, out_size, cudaMemcpyDeviceToHost, stream);\n",
        "\n",
        "    cout << \"CONV1 DONE\" << endl;\n",
        "}\n",
        "\n",
        "void call_conv2(ConvLayers &convlayer1, cudaStream_t stream, cudnnHandle_t CUDNN, float input[][224][224][10], float output[BATCH_SIZE][224][224][30])\n",
        "{\n",
        "    float *inputTensor;\n",
        "\tfloat *outputTensor;\n",
        "\tint inp_size = convlayer1.inDims.Height * convlayer1.inDims.Width * convlayer1.inDims.Channels * convlayer1.inDims.Batch * sizeof(float);\n",
        "    cudaMalloc(&inputTensor, inp_size);\n",
        "    cudaMemcpyAsync(inputTensor, input, inp_size, cudaMemcpyHostToDevice, stream);\n",
        "\n",
        "    void *d_workspace;\n",
        "    convlayer1.fwdProp(stream, CUDNN, inputTensor, outputTensor, d_workspace);\n",
        "\n",
        "    int out_size =  convlayer1.outDims.Height*convlayer1.outDims.Width*convlayer1.outDims.Channels*convlayer1.outDims.Batch*sizeof(float);\n",
        "    \n",
        "    cudaMemcpyAsync(output, outputTensor, out_size, cudaMemcpyDeviceToHost, stream);\n",
        "\n",
        "    cout << \"CONV2 DONE\" << endl;\n",
        "}\n",
        "\n",
        "\n",
        "void call_conv3(ConvLayers &convlayer1, cudaStream_t stream, cudnnHandle_t CUDNN, float input[][224][224][30], float output[BATCH_SIZE][224][224][30])\n",
        "{\n",
        "    float *inputTensor;\n",
        "\tfloat *outputTensor;\n",
        "\tint inp_size = convlayer1.inDims.Height * convlayer1.inDims.Width * convlayer1.inDims.Channels * convlayer1.inDims.Batch * sizeof(float);\n",
        "    cudaMalloc(&inputTensor, inp_size);\n",
        "    cudaMemcpyAsync(inputTensor, input, inp_size, cudaMemcpyHostToDevice, stream);\n",
        "\n",
        "    void *d_workspace;\n",
        "    convlayer1.fwdProp(stream, CUDNN, inputTensor, outputTensor, d_workspace);\n",
        "\n",
        "    int out_size =  convlayer1.outDims.Height*convlayer1.outDims.Width*convlayer1.outDims.Channels*convlayer1.outDims.Batch*sizeof(float);\n",
        "    \n",
        "    cudaMemcpyAsync(output, outputTensor, out_size, cudaMemcpyDeviceToHost, stream);\n",
        "\n",
        "    cout << \"CONV3 DONE\" << endl;\n",
        "}\n",
        "\n",
        "void call_conv4(ConvLayers &convlayer1, cudaStream_t stream, cudnnHandle_t CUDNN, float input[][224][224][30], float output[BATCH_SIZE][224][224][10])\n",
        "{\n",
        "    float *inputTensor;\n",
        "\tfloat *outputTensor;\n",
        "\tint inp_size = convlayer1.inDims.Height * convlayer1.inDims.Width * convlayer1.inDims.Channels * convlayer1.inDims.Batch * sizeof(float);\n",
        "    cudaMalloc(&inputTensor, inp_size);\n",
        "    cudaMemcpyAsync(inputTensor, input, inp_size, cudaMemcpyHostToDevice, stream);\n",
        "\n",
        "    void *d_workspace;\n",
        "    convlayer1.fwdProp(stream, CUDNN, inputTensor, outputTensor, d_workspace);\n",
        "\n",
        "    int out_size =  convlayer1.outDims.Height*convlayer1.outDims.Width*convlayer1.outDims.Channels*convlayer1.outDims.Batch*sizeof(float);\n",
        "    \n",
        "    cudaMemcpyAsync(output, outputTensor, out_size, cudaMemcpyDeviceToHost, stream);\n",
        "\n",
        "    cout << \"CONV4 DONE\" << endl;\n",
        "}\n",
        "\n",
        "void call_conv5(ConvLayers &convlayer1, cudaStream_t stream, cudnnHandle_t CUDNN, float input[][224][224][10], float output[BATCH_SIZE][224][224][3])\n",
        "{\n",
        "    float *inputTensor;\n",
        "\tfloat *outputTensor;\n",
        "\tint inp_size = convlayer1.inDims.Height * convlayer1.inDims.Width * convlayer1.inDims.Channels * convlayer1.inDims.Batch * sizeof(float);\n",
        "    cudaMalloc(&inputTensor, inp_size);\n",
        "    cudaMemcpyAsync(inputTensor, input, inp_size, cudaMemcpyHostToDevice, stream);\n",
        "\n",
        "    void *d_workspace;\n",
        "    convlayer1.fwdProp(stream, CUDNN, inputTensor, outputTensor, d_workspace);\n",
        "\n",
        "    int out_size =  convlayer1.outDims.Height*convlayer1.outDims.Width*convlayer1.outDims.Channels*convlayer1.outDims.Batch*sizeof(float);\n",
        "    \n",
        "    cudaMemcpyAsync(output, outputTensor, out_size, cudaMemcpyDeviceToHost, stream);\n",
        "\n",
        "    cout << \"CONV5 DONE\" << endl;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "void execute(vector<Kernel> C)\n",
        "{\n",
        "    \n",
        "    for(int i=0; i<C.size();i++){\n",
        "        Kernel K = C[i];\n",
        "        if(K.layer == 1)\n",
        "            call_conv1(convlayer1, K.stream, K.cudnn, img, output1);\n",
        "        else if(K.layer == 2)\n",
        "            call_conv2(convlayer2, K.stream, K.cudnn, output1, output2);\n",
        "        else if(K.layer == 3)\n",
        "            call_conv3(convlayer3, K.stream, K.cudnn, output2, output3);\n",
        "        else if(K.layer == 4)\n",
        "            call_conv4(convlayer4, K.stream, K.cudnn, output3, output4);\n",
        "        else\n",
        "            call_conv5(convlayer5, K.stream, K.cudnn, output4, output5);\n",
        "    }\n",
        "\t\n",
        "}\n",
        "\n",
        "//channel, height, width\n",
        "int main(){\n",
        "  for(int i=0;i<BATCH_SIZE;++i){\n",
        "    for(int j=0; j<224;++j){\n",
        "      for(int k=0;k<224;++k){\n",
        "        for(int l=0;l<3;++l){\n",
        "          img[i][j][k][l]=0.001;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "\tconvDim_t InputDims1 = setConvSpecs(224, 224, 3, BATCH_SIZE);\n",
        "\tkernelDim_t layerKernel1 = setKernelSpecs(0,3,3,1,1,1,1,1,1);\n",
        "\tconvDim_t OutputDims1 = setConvSpecs(224, 224, 10, BATCH_SIZE);\n",
        "\tconvlayer1.set(1, InputDims1, layerKernel1, OutputDims1);\n",
        "\tconvlayer1.buildConvLayer();\n",
        "\t\n",
        "\tconvDim_t InputDims2 = setConvSpecs(224, 224, 10, BATCH_SIZE);\n",
        "\tkernelDim_t layerKernel2 = setKernelSpecs(0,3,3,1,1,1,1,1,1);\n",
        "\tconvDim_t OutputDims2 = setConvSpecs(224, 224, 30, BATCH_SIZE);\n",
        "\tconvlayer2.set(2, InputDims2, layerKernel2, OutputDims2);\n",
        "\tconvlayer2.buildConvLayer();\n",
        "\n",
        "\tconvDim_t InputDims3 = setConvSpecs(224, 224, 30, BATCH_SIZE);\n",
        "\tkernelDim_t layerKernel3 = setKernelSpecs(0,3,3,1,1,1,1,1,1);\n",
        "\tconvDim_t OutputDims3 = setConvSpecs(224, 224, 30, BATCH_SIZE);\n",
        "\tconvlayer3.set(3, InputDims3, layerKernel3, OutputDims3);\n",
        "\tconvlayer3.buildConvLayer();\n",
        "\n",
        "\tconvDim_t InputDims4 = setConvSpecs(224, 224, 30, BATCH_SIZE);\n",
        "\tkernelDim_t layerKernel4 = setKernelSpecs(0,3,3,1,1,1,1,1,1);\n",
        "\tconvDim_t OutputDims4 = setConvSpecs(224, 224, 10, BATCH_SIZE);\n",
        "\tconvlayer4.set(4, InputDims4, layerKernel4, OutputDims4);\n",
        "\tconvlayer4.buildConvLayer();\n",
        "\n",
        "\tconvDim_t InputDims5 = setConvSpecs(224, 224, 10, BATCH_SIZE);\n",
        "\tkernelDim_t layerKernel5 = setKernelSpecs(0,3,3,1,1,1,1,1,1);\n",
        "\tconvDim_t OutputDims5 = setConvSpecs(224, 224, 3, BATCH_SIZE);\n",
        "\tconvlayer5.set(5, InputDims5, layerKernel5, OutputDims5);\n",
        "\tconvlayer5.buildConvLayer();\n",
        "\n",
        "\tint nstreams = 4;\n",
        "\tcudaStream_t stream[nstreams];\n",
        "\tcudnnHandle_t cudnn[nstreams];\n",
        "    cublasHandle_t cublas[nstreams];\n",
        "\n",
        "  \n",
        "\n",
        "\tfor(int i=0;i<nstreams;++i){\n",
        "\t\tcudaStreamCreate(&stream[i]);\n",
        "\t\tcheckCUDNN(cudnnCreate(&cudnn[i]));\n",
        "\t\tcudnnSetStream(cudnn[i], stream[i]);\n",
        "        cublasSetStream(cublas[i], stream[i]);\n",
        "\t}\n",
        "\n",
        "for(int j=1;j<=nstreams;j++){\n",
        "    vector<Kernel> inp;\n",
        "    SET pq;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    \n",
        "    for(int i=0;i<NUM_BATCHES;i++)\n",
        "    {\n",
        "          Kernel k(stream[i%j], cublas[i%j], cudnn[i%j], 1, 0, 30+i, 1.2, i);   \n",
        "          inp.push_back(k);\n",
        "    }\n",
        "      pq = enqueue(pq, inp);\n",
        "      while(!pq.empty()){\n",
        "          pq = dequeue(pq);\n",
        "      }\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    float elapsedtime;\n",
        "    cudaEventElapsedTime(&elapsedtime, start, stop);\n",
        "    cout<<\"Num Streams\"<<j << \"ELAPSED TIME: \" << elapsedtime << endl;\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "}\n",
        "    \n",
        "\n",
        "    cout<<output1[0][0][0][0]<<endl;\n",
        "    cout<<output2[0][0][0][0]<<endl;\n",
        "    cout<<output3[0][0][0][0]<<endl;\n",
        "    cout<<output4[0][0][0][0]<<endl;\n",
        "    cout<<output5[0][0][0][0]<<endl;\n",
        "\treturn 0;\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/alex.cu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7vwioPpZzMY"
      },
      "source": [
        "!nvcc /content/src/alex.cu `pkg-config --cflags --libs opencv` -lcudnn -lcublas -lopencv_imgcodecs -lopencv_imgproc -lopencv_core -pg -std=c++11 -o /content/alex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXAUjdhSwBgN",
        "outputId": "f527555e-7bc0-4c79-8dd2-edae923f749b"
      },
      "source": [
        "!/content/alex "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enqueue ID: 0 1\n",
            "Enqueue ID: 1 1\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 1\n",
            "Dequeu ID: 1 1\n",
            "Single kernel execution\n",
            "CONV1 DONE\n",
            "Priority:10\n",
            "Enqueue ID: 0 2\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 1 1\n",
            "Dequeu ID: 0 2\n",
            "Single kernel execution\n",
            "CONV1 DONE\n",
            "Priority:6\n",
            "Enqueue ID: 1 2\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 2\n",
            "Dequeu ID: 1 2\n",
            "Single kernel execution\n",
            "CONV2 DONE\n",
            "Priority:3\n",
            "Enqueue ID: 0 3\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 1 2\n",
            "Dequeu ID: 0 3\n",
            "Single kernel execution\n",
            "CONV2 DONE\n",
            "Priority:-1\n",
            "Enqueue ID: 1 3\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 3\n",
            "Dequeu ID: 1 3\n",
            "Single kernel execution\n",
            "CONV3 DONE\n",
            "Priority:-4\n",
            "Enqueue ID: 0 4\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 1 3\n",
            "Dequeu ID: 0 4\n",
            "Single kernel execution\n",
            "CONV3 DONE\n",
            "Priority:-8\n",
            "Enqueue ID: 1 4\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 4\n",
            "Dequeu ID: 1 4\n",
            "Single kernel execution\n",
            "CONV4 DONE\n",
            "Priority:-11\n",
            "Enqueue ID: 0 5\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 1 4\n",
            "Dequeu ID: 0 5\n",
            "Single kernel execution\n",
            "CONV4 DONE\n",
            "Priority:-15\n",
            "Enqueue ID: 1 5\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 5\n",
            "Dequeu ID: 1 5\n",
            "Single kernel execution\n",
            "CONV5 DONE\n",
            "Priority:-18\n",
            "Printing Queue\n",
            "1\n",
            "Dequeu ID: 1 5\n",
            "Single kernel execution\n",
            "CONV5 DONE\n",
            "Priority:-22\n",
            "Num Streams1ELAPSED TIME: 457.939\n",
            "Enqueue ID: 0 1\n",
            "Enqueue ID: 1 1\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 1\n",
            "Dequeu ID: 1 1\n",
            "Single kernel execution\n",
            "CONV1 DONE\n",
            "Priority:10\n",
            "Enqueue ID: 0 2\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 1 1\n",
            "Dequeu ID: 0 2\n",
            "Single kernel execution\n",
            "CONV1 DONE\n",
            "Priority:6\n",
            "Enqueue ID: 1 2\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 2\n",
            "Dequeu ID: 1 2\n",
            "Single kernel execution\n",
            "CONV2 DONE\n",
            "Priority:3\n",
            "Enqueue ID: 0 3\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 1 2\n",
            "Dequeu ID: 0 3\n",
            "Single kernel execution\n",
            "CONV2 DONE\n",
            "Priority:-1\n",
            "Enqueue ID: 1 3\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 3\n",
            "Dequeu ID: 1 3\n",
            "Single kernel execution\n",
            "CONV3 DONE\n",
            "Priority:-4\n",
            "Enqueue ID: 0 4\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 1 3\n",
            "Dequeu ID: 0 4\n",
            "Single kernel execution\n",
            "CONV3 DONE\n",
            "Priority:-8\n",
            "Enqueue ID: 1 4\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 4\n",
            "Dequeu ID: 1 4\n",
            "Single kernel execution\n",
            "CONV4 DONE\n",
            "Priority:-11\n",
            "Enqueue ID: 0 5\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 1 4\n",
            "Dequeu ID: 0 5\n",
            "Single kernel execution\n",
            "CONV4 DONE\n",
            "Priority:-15\n",
            "Enqueue ID: 1 5\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 5\n",
            "Dequeu ID: 1 5\n",
            "Single kernel execution\n",
            "CONV5 DONE\n",
            "Priority:-18\n",
            "Printing Queue\n",
            "1\n",
            "Dequeu ID: 1 5\n",
            "Single kernel execution\n",
            "CONV5 DONE\n",
            "Priority:-22\n",
            "Num Streams2ELAPSED TIME: 304.322\n",
            "Enqueue ID: 0 1\n",
            "Enqueue ID: 1 1\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 1\n",
            "Dequeu ID: 1 1\n",
            "Single kernel execution\n",
            "CONV1 DONE\n",
            "Priority:10\n",
            "Enqueue ID: 0 2\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 1 1\n",
            "Dequeu ID: 0 2\n",
            "Single kernel execution\n",
            "CONV1 DONE\n",
            "Priority:6\n",
            "Enqueue ID: 1 2\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 2\n",
            "Dequeu ID: 1 2\n",
            "Single kernel execution\n",
            "CONV2 DONE\n",
            "Priority:3\n",
            "Enqueue ID: 0 3\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 1 2\n",
            "Dequeu ID: 0 3\n",
            "Single kernel execution\n",
            "CONV2 DONE\n",
            "Priority:-1\n",
            "Enqueue ID: 1 3\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 3\n",
            "Dequeu ID: 1 3\n",
            "Single kernel execution\n",
            "CONV3 DONE\n",
            "Priority:-4\n",
            "Enqueue ID: 0 4\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 1 3\n",
            "Dequeu ID: 0 4\n",
            "Single kernel execution\n",
            "CONV3 DONE\n",
            "Priority:-8\n",
            "Enqueue ID: 1 4\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 4\n",
            "Dequeu ID: 1 4\n",
            "Single kernel execution\n",
            "CONV4 DONE\n",
            "Priority:-11\n",
            "Enqueue ID: 0 5\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 1 4\n",
            "Dequeu ID: 0 5\n",
            "Single kernel execution\n",
            "CONV4 DONE\n",
            "Priority:-15\n",
            "Enqueue ID: 1 5\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 5\n",
            "Dequeu ID: 1 5\n",
            "Single kernel execution\n",
            "CONV5 DONE\n",
            "Priority:-18\n",
            "Printing Queue\n",
            "1\n",
            "Dequeu ID: 1 5\n",
            "Single kernel execution\n",
            "CONV5 DONE\n",
            "Priority:-22\n",
            "Num Streams3ELAPSED TIME: 275.556\n",
            "Enqueue ID: 0 1\n",
            "Enqueue ID: 1 1\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 1\n",
            "Dequeu ID: 1 1\n",
            "Single kernel execution\n",
            "CONV1 DONE\n",
            "Priority:10\n",
            "Enqueue ID: 0 2\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 1 1\n",
            "Dequeu ID: 0 2\n",
            "Single kernel execution\n",
            "CONV1 DONE\n",
            "Priority:6\n",
            "Enqueue ID: 1 2\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 2\n",
            "Dequeu ID: 1 2\n",
            "Single kernel execution\n",
            "CONV2 DONE\n",
            "Priority:3\n",
            "Enqueue ID: 0 3\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 1 2\n",
            "Dequeu ID: 0 3\n",
            "Single kernel execution\n",
            "CONV2 DONE\n",
            "Priority:-1\n",
            "Enqueue ID: 1 3\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 3\n",
            "Dequeu ID: 1 3\n",
            "Single kernel execution\n",
            "CONV3 DONE\n",
            "Priority:-4\n",
            "Enqueue ID: 0 4\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 1 3\n",
            "Dequeu ID: 0 4\n",
            "Single kernel execution\n",
            "CONV3 DONE\n",
            "Priority:-8\n",
            "Enqueue ID: 1 4\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 4\n",
            "Dequeu ID: 1 4\n",
            "Single kernel execution\n",
            "CONV4 DONE\n",
            "Priority:-11\n",
            "Enqueue ID: 0 5\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 1 4\n",
            "Dequeu ID: 0 5\n",
            "Single kernel execution\n",
            "CONV4 DONE\n",
            "Priority:-15\n",
            "Enqueue ID: 1 5\n",
            "Printing Queue\n",
            "2\n",
            "Dequeu ID: 0 5\n",
            "Dequeu ID: 1 5\n",
            "Single kernel execution\n",
            "CONV5 DONE\n",
            "Priority:-18\n",
            "Printing Queue\n",
            "1\n",
            "Dequeu ID: 1 5\n",
            "Single kernel execution\n",
            "CONV5 DONE\n",
            "Priority:-22\n",
            "Num Streams4ELAPSED TIME: 273.338\n",
            "0.006\n",
            "0.1875\n",
            "19.0125\n",
            "2067.19\n",
            "77760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgwz1zMK19KP",
        "outputId": "2d7b1360-a96a-4af0-c926-f32c09718371"
      },
      "source": [
        "! lscpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Architecture:        x86_64\n",
            "CPU op-mode(s):      32-bit, 64-bit\n",
            "Byte Order:          Little Endian\n",
            "CPU(s):              2\n",
            "On-line CPU(s) list: 0,1\n",
            "Thread(s) per core:  2\n",
            "Core(s) per socket:  1\n",
            "Socket(s):           1\n",
            "NUMA node(s):        1\n",
            "Vendor ID:           GenuineIntel\n",
            "CPU family:          6\n",
            "Model:               79\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "Stepping:            0\n",
            "CPU MHz:             2199.998\n",
            "BogoMIPS:            4399.99\n",
            "Hypervisor vendor:   KVM\n",
            "Virtualization type: full\n",
            "L1d cache:           32K\n",
            "L1i cache:           32K\n",
            "L2 cache:            256K\n",
            "L3 cache:            56320K\n",
            "NUMA node0 CPU(s):   0,1\n",
            "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5biCCaah17wM"
      },
      "source": [
        "!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCeqG-HumpW2",
        "outputId": "747c6c32-f478-42f8-e060-a76cca3ffc5b"
      },
      "source": [
        "!nvprof /content/alex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output image size 1 X 28 X 28 X 64\n",
            "==3684== NVPROF is profiling process 3684, command: /content/alex\n",
            "Output image size 1 X 14 X 14 X 192\n",
            "Output image size 1 X 14 X 14 X 384\n",
            "Output image size 1 X 14 X 14 X 256\n",
            "Output image size 1 X 7 X 7 X 256\n",
            "Output image size 4096\n",
            "Output image size 4096\n",
            "Output image size 1000\n",
            "BUILT ALL LAYERS\n",
            "K1:0.6 21 50\n",
            "Printing Queue\n",
            "ID: 0\n",
            "ID: 1\n",
            "ID: 2\n",
            "ID: 3\n",
            "ID: 4\n",
            "ID: 5\n",
            "ID: 6\n",
            "ID: 7\n",
            "ID: 8\n",
            "ID: 9\n",
            "Total TBratio 1.2\n",
            "CONV1 DONE\n",
            "CONV1 DONE\n",
            "Execution size: 2\n",
            "Priority:15\n",
            "Priority:16\n",
            "Printing Queue\n",
            "ID: 2\n",
            "ID: 3\n",
            "ID: 4\n",
            "ID: 5\n",
            "ID: 6\n",
            "ID: 7\n",
            "ID: 8\n",
            "ID: 9\n",
            "Total TBratio 1.2\n",
            "CONV1 DONE\n",
            "CONV1 DONE\n",
            "Execution size: 2\n",
            "Priority:12\n",
            "Priority:13\n",
            "Printing Queue\n",
            "ID: 4\n",
            "ID: 5\n",
            "ID: 6\n",
            "ID: 7\n",
            "ID: 8\n",
            "ID: 9\n",
            "Total TBratio 1.2\n",
            "CONV1 DONE\n",
            "CONV1 DONE\n",
            "Execution size: 2\n",
            "Priority:9\n",
            "Priority:10\n",
            "Printing Queue\n",
            "ID: 6\n",
            "ID: 7\n",
            "ID: 8\n",
            "ID: 9\n",
            "Total TBratio 1.2\n",
            "CONV1 DONE\n",
            "CONV1 DONE\n",
            "Execution size: 2\n",
            "Priority:6\n",
            "Priority:7\n",
            "Printing Queue\n",
            "ID: 8\n",
            "ID: 9\n",
            "ID: 7\n",
            "Total TBratio 1.2\n",
            "CONV1 DONE\n",
            "CONV1 DONE\n",
            "Execution size: 2\n",
            "Priority:3\n",
            "Priority:4\n",
            "Printing Queue\n",
            "ID: 7\n",
            "ID: 8\n",
            "ID: 9\n",
            "Total TBratio 1.2\n",
            "CONV2 DONE\n",
            "CONV2 DONE\n",
            "Execution size: 2\n",
            "Priority:0\n",
            "Priority:1\n",
            "Printing Queue\n",
            "ID: 9\n",
            "ID: 7\n",
            "ID: 8\n",
            "Total TBratio 1.2\n",
            "CONV2 DONE\n",
            "CONV3 DONE\n",
            "Execution size: 2\n",
            "Priority:-3\n",
            "Priority:-2\n",
            "Printing Queue\n",
            "ID: 8\n",
            "ID: 9\n",
            "ID: 7\n",
            "Total TBratio 1.2\n",
            "CONV3 DONE\n",
            "CONV3 DONE\n",
            "Execution size: 2\n",
            "Priority:-6\n",
            "Priority:-5\n",
            "Printing Queue\n",
            "ID: 7\n",
            "ID: 8\n",
            "ID: 9\n",
            "Total TBratio 1.2\n",
            "CONV4 DONE\n",
            "CONV4 DONE\n",
            "Execution size: 2\n",
            "Priority:-9\n",
            "Priority:-8\n",
            "Printing Queue\n",
            "ID: 9\n",
            "ID: 7\n",
            "ID: 8\n",
            "Total TBratio 1.2\n",
            "CONV4 DONE\n",
            "CONV5 DONE\n",
            "Execution size: 2\n",
            "Priority:-12\n",
            "Priority:-11\n",
            "Printing Queue\n",
            "ID: 8\n",
            "ID: 9\n",
            "ID: 7\n",
            "Total TBratio 1.2\n",
            "CONV5 DONE\n",
            "CONV5 DONE\n",
            "Execution size: 2\n",
            "Priority:-15\n",
            "Priority:-14\n",
            "Printing Queue\n",
            "ID: 7\n",
            "ID: 8\n",
            "ID: 9\n",
            "Total TBratio 1.2\n",
            "FC1 DONE\n",
            "FC1 DONE\n",
            "Execution size: 2\n",
            "Priority:-18\n",
            "Priority:-17\n",
            "Printing Queue\n",
            "ID: 9\n",
            "ID: 7\n",
            "ID: 8\n",
            "Total TBratio 0.8\n",
            "Total TBratio 1\n",
            "FC1 DONE\n",
            "FC2 DONE\n",
            "FC2 DONE\n",
            "Execution size: 3\n",
            "Priority:-21\n",
            "Priority:-20\n",
            "Priority:-19\n",
            "Printing Queue\n",
            "ID: 9\n",
            "FC2 DONE\n",
            "Execution size: 1\n",
            "Priority:-23\n",
            "Total Time: 27960\n",
            "==3684== Profiling application: /content/alex\n",
            "==3684== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==3684== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sODQfBGcx9cO"
      },
      "source": [
        "# %%cuda --name yolo.cu\n",
        "# #include<bits/stdc++.h>\n",
        "# using namespace std;\n",
        "\n",
        "# // Initialize the parameters\n",
        "# float confThreshold = 0.5; // Confidence threshold\n",
        "# float nmsThreshold = 0.4;  // Non-maximum suppression threshold\n",
        "# int inpWidth = 416;        // Width of network's input image\n",
        "# int inpHeight = 416;       // Height of network's input image\n",
        "\n",
        "# // Load names of classes\n",
        "# string classesFile = \"/content/drive/My Drive/hp3_data/cfg/coco.names\";\n",
        "# ifstream ifs(classesFile.c_str());\n",
        "# string line;\n",
        "# while (getline(ifs, line)) classes.push_back(line);\n",
        "\n",
        "# // Give the configuration and weight files for the model\n",
        "# String modelConfiguration = \"/content/drive/My Drive/hp3_data/cfg/yolov3.cfg\";\n",
        "# String modelWeights = \"/content/drive/My Drive/hp3_data/cfg/yolov3.weights\";\n",
        "\n",
        "# // Load the network\n",
        "# Net net = readNetFromDarknet(modelConfiguration, modelWeights);\n",
        "# net.setPreferableBackend(DNN_BACKEND_OPENCV);\n",
        "# net.setPreferableTarget(DNN_TARGET_CPU);\n",
        "\n",
        "# outputFile = \"/content/drive/My Drive/hp3_data/yolo_out_cpp.avi\";\n",
        "# if (parser.has(\"image\"))\n",
        "# {\n",
        "#     // Open the image file\n",
        "#     str = parser.get<String>(\"image\");\n",
        "#     ifstream ifile(str);\n",
        "#     if (!ifile) throw(\"error\");\n",
        "#     cap.open(str);\n",
        "#     str.replace(str.end()-4, str.end(), \"_yolo_out_cpp.jpg\");\n",
        "#     outputFile = str;\n",
        "# }\n",
        "# else if (parser.has(\"video\"))\n",
        "# {\n",
        "#     // Open the video file\n",
        "#     str = parser.get<String>(\"video\");\n",
        "#     ifstream ifile(str);\n",
        "#     if (!ifile) throw(\"error\");\n",
        "#     cap.open(str);\n",
        "#     str.replace(str.end()-4, str.end(), \"_yolo_out_cpp.avi\");\n",
        "#     outputFile = str;\n",
        "# }\n",
        "# // Open the webcaom\n",
        "# else cap.open(parser.get<int>(\"device\"));\n",
        "\n",
        "# catch(...) {\n",
        "#     cout << \"Could not open the input image/video stream\" << endl;\n",
        "#     return 0;\n",
        "# }\n",
        "\n",
        "# // Get the video writer initialized to save the output video\n",
        "# if (!parser.has(\"image\")) {\n",
        "#     video.open(outputFile, VideoWriter::fourcc('M','J','P','G'), 28, Size(cap.get(CAP_PROP_FRAME_WIDTH), cap.get(CAP_PROP_FRAME_HEIGHT)));\n",
        "# }\n",
        "\n",
        "\n",
        "# // Process frames.\n",
        "# while (waitKey(1) < 0)\n",
        "# {\n",
        "#     // get frame from the video\n",
        "#     cap >> frame;\n",
        "\n",
        "#     // Stop the program if reached end of video\n",
        "#     if (frame.empty()) {\n",
        "#         cout << \"Done processing !!!\" << endl;\n",
        "#         cout << \"Output file is stored as \" << outputFile << endl;\n",
        "#         waitKey(3000);\n",
        "#         break;\n",
        "#     }\n",
        "#     // Create a 4D blob from a frame.\n",
        "#     blobFromImage(frame, blob, 1/255.0, cv::Size(inpWidth, inpHeight), Scalar(0,0,0), true, false);\n",
        "        \n",
        "#     //Sets the input to the network\n",
        "#     net.setInput(blob);\n",
        "    \n",
        "#     // Runs the forward pass to get output of the output layers\n",
        "#     vector<Mat> outs;\n",
        "#     net.forward(outs, getOutputsNames(net));\n",
        "        \n",
        "#     // Remove the bounding boxes with low confidence\n",
        "#     postprocess(frame, outs);\n",
        "        \n",
        "#     // Put efficiency information. The function getPerfProfile returns the overall time for inference(t) and the timings for each of the layers(in layersTimes)\n",
        "#     vector<double> layersTimes;\n",
        "#     double freq = getTickFrequency() / 1000;\n",
        "#     double t = net.getPerfProfile(layersTimes) / freq;\n",
        "#     string label = format(\"Inference time for a frame : %.2f ms\", t);\n",
        "#     putText(frame, label, Point(0, 15), FONT_HERSHEY_SIMPLEX, 0.5, Scalar(0, 0, 255));\n",
        "        \n",
        "#     // Write the frame with the detection boxes\n",
        "#     Mat detectedFrame;\n",
        "#     frame.convertTo(detectedFrame, CV_8U);\n",
        "#     if (parser.has(\"image\")) imwrite(outputFile, detectedFrame);\n",
        "#     else video.write(detectedFrame);\n",
        "        \n",
        "#     imshow(kWinName, frame);\n",
        "        \n",
        "# }\n",
        "\n",
        "# // Get the names of the output layers\n",
        "# vector<String> getOutputsNames(const Net& net)\n",
        "# {\n",
        "#     static vector<String> names;\n",
        "#     if (names.empty())\n",
        "#     {\n",
        "#         //Get the indices of the output layers, i.e. the layers with unconnected outputs\n",
        "#         vector<int> outLayers = net.getUnconnectedOutLayers();\n",
        "        \n",
        "#         //get the names of all the layers in the network\n",
        "#         vector<String> layersNames = net.getLayerNames();\n",
        "        \n",
        "#         // Get the names of the output layers in names\n",
        "#         names.resize(outLayers.size());\n",
        "#         for (size_t i = 0; i < outLayers.size(); ++i)\n",
        "#         names[i] = layersNames[outLayers[i] - 1];\n",
        "#     }\n",
        "#     return names;\n",
        "# }\n",
        "\n",
        "\n",
        "# // Remove the bounding boxes with low confidence using non-maxima suppression\n",
        "# void postprocess(Mat& frame, const vector<Mat>& outs)\n",
        "# {\n",
        "#     vector<int> classIds;\n",
        "#     vector<float> confidences;\n",
        "#     vector<Rect> boxes;\n",
        "    \n",
        "#     for (size_t i = 0; i < outs.size(); ++i)\n",
        "#     {\n",
        "#         // Scan through all the bounding boxes output from the network and keep only the\n",
        "#         // ones with high confidence scores. Assign the box's class label as the class\n",
        "#         // with the highest score for the box.\n",
        "#         float* data = (float*)outs[i].data;\n",
        "#         for (int j = 0; j < outs[i].rows; ++j, data += outs[i].cols)\n",
        "#         {\n",
        "#             Mat scores = outs[i].row(j).colRange(5, outs[i].cols);\n",
        "#             Point classIdPoint;\n",
        "#             double confidence;\n",
        "#             // Get the value and location of the maximum score\n",
        "#             minMaxLoc(scores, 0, &confidence, 0, &classIdPoint);\n",
        "#             if (confidence > confThreshold)\n",
        "#             {\n",
        "#                 int centerX = (int)(data[0] * frame.cols);\n",
        "#                 int centerY = (int)(data[1] * frame.rows);\n",
        "#                 int width = (int)(data[2] * frame.cols);\n",
        "#                 int height = (int)(data[3] * frame.rows);\n",
        "#                 int left = centerX - width / 2;\n",
        "#                 int top = centerY - height / 2;\n",
        "                \n",
        "#                 classIds.push_back(classIdPoint.x);\n",
        "#                 confidences.push_back((float)confidence);\n",
        "#                 boxes.push_back(Rect(left, top, width, height));\n",
        "#             }\n",
        "#         }\n",
        "#     }\n",
        "    \n",
        "#     // Perform non maximum suppression to eliminate redundant overlapping boxes with\n",
        "#     // lower confidences\n",
        "#     vector<int> indices;\n",
        "#     NMSBoxes(boxes, confidences, confThreshold, nmsThreshold, indices);\n",
        "#     for (size_t i = 0; i < indices.size(); ++i)\n",
        "#     {\n",
        "#         int idx = indices[i];\n",
        "#         Rect box = boxes[idx];\n",
        "#         drawPred(classIds[idx], confidences[idx], box.x, box.y,\n",
        "#                  box.x + box.width, box.y + box.height, frame);\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# // Draw the predicted bounding box\n",
        "# void drawPred(int classId, float conf, int left, int top, int right, int bottom, Mat& frame)\n",
        "# {\n",
        "#     //Draw a rectangle displaying the bounding box\n",
        "#     rectangle(frame, Point(left, top), Point(right, bottom), Scalar(255, 178, 50), 3);\n",
        "    \n",
        "#     //Get the label for the class name and its confidence\n",
        "#     string label = format(\"%.2f\", conf);\n",
        "#     if (!classes.empty())\n",
        "#     {\n",
        "#         CV_Assert(classId < (int)classes.size());\n",
        "#         label = classes[classId] + \":\" + label;\n",
        "#     }\n",
        "    \n",
        "#     //Display the label at the top of the bounding box\n",
        "#     int baseLine;\n",
        "#     Size labelSize = getTextSize(label, FONT_HERSHEY_SIMPLEX, 0.5, 1, &baseLine);\n",
        "#     top = max(top, labelSize.height);\n",
        "#     rectangle(frame, Point(left, top - round(1.5*labelSize.height)), Point(left + round(1.5*labelSize.width), top + baseLine), Scalar(255, 255, 255), FILLED);\n",
        "#     putText(frame, label, Point(left, top), FONT_HERSHEY_SIMPLEX, 0.75, Scalar(0,0,0),1);\n",
        "# }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Eemuk5JKvN48",
        "outputId": "a2100ce6-de74-4e9b-d1f1-9f28190901e5"
      },
      "source": [
        "%%cuda --name alex.cu\n",
        "#include <cudnn.h>\n",
        "#include <cublas_v2.h>\n",
        "#include <cudaProfiler.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cstdlib>\n",
        "#include <cassert>\n",
        "#include <cstdlib>\n",
        "#include <iostream>\n",
        "#include <string>\n",
        "#include <random>\n",
        "#include <cmath>\n",
        "#include <stdio.h>\n",
        "#include <bits/stdc++.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "#define BATCH_SIZE 1\n",
        "#define OVERLAP_POOLING 1\n",
        "#define BIAS_INIT_VAL 0.001\n",
        "#define MAX_THREADS_PER_BLOCK 1024 // according to GTX 1050 Ti\n",
        "#define LAYERS 7\n",
        "#define CONST_TIME 3\n",
        "\n",
        "int roundUp(int num, int den)\n",
        "{\n",
        "\n",
        "  return((num + den - 1 )/(den));\n",
        "\n",
        "}\n",
        "\n",
        "struct convDim_t{\n",
        "\n",
        "  int Height;\n",
        "  int Width;\n",
        "  int Channels;\n",
        "  int Batch;\n",
        "};\n",
        "\n",
        "struct poolDim_t{\n",
        "\n",
        "  int Height;\n",
        "  int Width;\n",
        "  int padHeight;\n",
        "  int padWidth;\n",
        "  int strideHeight;\n",
        "  int strideWidth;\n",
        "};\n",
        "\n",
        "struct kernelDim_t{\n",
        "\n",
        "  int kernelSize;\n",
        "  int kernelHeight;\n",
        "  int kernelWidth;\n",
        "  int strideHeight;\n",
        "  int strideWidth;\n",
        "  int padHeight;\n",
        "  int padWidth;\n",
        "  int dilationHeight;\n",
        "  int dilationWidth;\n",
        "};\n",
        "\n",
        "convDim_t setConvSpecs(int ht, int wd, int ch, int bt){\n",
        "\n",
        "  convDim_t temp;\n",
        "  temp.Height = ht;\n",
        "  temp.Width = wd;\n",
        "  temp.Channels = ch;\n",
        "  temp.Batch = bt;\n",
        "\n",
        "  return temp;\n",
        "}\n",
        "\n",
        "kernelDim_t setKernelSpecs(int size, int fheight, int fwidth, int sheight, int swidth, int pheight, int pwidth, int dheight, int dwidth){\n",
        "\n",
        "  kernelDim_t layerKernel;\n",
        "  layerKernel.kernelSize = size;\n",
        "  layerKernel.kernelHeight = fheight;\n",
        "  layerKernel.kernelWidth = fwidth;\n",
        "  layerKernel.strideHeight = sheight;\n",
        "  layerKernel.strideWidth = swidth;\n",
        "  layerKernel.padHeight = pheight;\n",
        "  layerKernel.padWidth = pwidth;\n",
        "  layerKernel.dilationHeight = dheight;\n",
        "  layerKernel.dilationWidth = dwidth;\n",
        "\n",
        "  return layerKernel;\n",
        "}\n",
        "\n",
        "poolDim_t setPoolSpecs(bool flagOverlap){\n",
        "\n",
        "  poolDim_t poolDims;\n",
        "\n",
        "  if(flagOverlap){\n",
        "    poolDims.Height = 3;\n",
        "    poolDims.Width = 3;\n",
        "    poolDims.padHeight = 0;\n",
        "    poolDims.padWidth = 0;\n",
        "    poolDims.strideHeight = 2;\n",
        "    poolDims.strideWidth = 2;  \n",
        "  }\n",
        "  else{\n",
        "    poolDims.Height = 2;\n",
        "    poolDims.Width = 2;\n",
        "    poolDims.padHeight = 1;\n",
        "    poolDims.padWidth = 1;\n",
        "    poolDims.strideHeight = 2;\n",
        "    poolDims.strideWidth = 2;\n",
        "  }\n",
        "\n",
        "  return poolDims;\n",
        "  \n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "#define checkCUDNN(expression)                             \\\n",
        "{                                                          \\\n",
        "  cudnnStatus_t status = (expression);                     \\\n",
        "  if (status != CUDNN_STATUS_SUCCESS) {                    \\\n",
        "    std::cerr << \"Error on line \" << __LINE__ << \": \"      \\\n",
        "              << cudnnGetErrorString(status) << std::endl; \\\n",
        "    std::exit(EXIT_FAILURE);                               \\\n",
        "  }                                                        \\\n",
        "}\n",
        "\n",
        "\n",
        "float alpha = 1.0;\n",
        "float beta = 0.5;\n",
        "\n",
        "\n",
        "\n",
        "class ConvLayers{\n",
        "    \n",
        "public:\n",
        "    float *kernelTensor;\n",
        "    float *biasTensor;\t\t\n",
        "    int layerIndex;\n",
        "    float alph, bet;\n",
        "\n",
        "    cudnnTensorFormat_t TensorFormat;\n",
        "    cudnnDataType_t DataType;\n",
        "    cudnnConvolutionMode_t ConvMode;\n",
        "    cudnnActivationMode_t ActivationMode;\n",
        "    cudnnPoolingMode_t PoolingMode;\n",
        "\n",
        "    convDim_t outDims;\n",
        "    convDim_t inDims;\n",
        "    kernelDim_t kernelDims;\n",
        "    poolDim_t poolDims;\n",
        "\n",
        "    random_device rd{};\n",
        "    mt19937 gen{rd()};  \n",
        "    normal_distribution<> d{0,1}; \n",
        "\n",
        "    /**\n",
        "    float* conv_output{nullptr}; // output of convolution operation\n",
        "    float* poolTensor{nullptr};  // output of pooling layer, if exists\n",
        "    void* d_workspace{nullptr};\n",
        "    size_t workspaceBytes{0};\n",
        "    **/\n",
        "\n",
        "    \n",
        "\n",
        "    int convOutDimHeight{0}, convOutDimWidth{0}, convOutDimChannels{0}, convOutDimBatchSize{0};\n",
        "    int poolOutBatchSize{0}, poolOutChannels{0}, poolOutHeight{0}, poolOutWidth{0};\n",
        "\n",
        "    bool POOL;  // True if pooling is to be done in this layer, otherwise False\n",
        "\n",
        "    cudnnTensorDescriptor_t input_descriptor;\n",
        "    cudnnFilterDescriptor_t kernel_descriptor;\n",
        "    cudnnConvolutionDescriptor_t convolution_descriptor;\n",
        "    cudnnTensorDescriptor_t bias_descriptor;\n",
        "    cudnnTensorDescriptor_t convOutput_descriptor;\n",
        "    cudnnConvolutionFwdAlgo_t convolution_algorithm;\n",
        "    cudnnActivationDescriptor_t activation_descriptor;\n",
        "    cudnnPoolingDescriptor_t pooling_descriptor;\n",
        "    cudnnTensorDescriptor_t poolTensor_descriptor;\n",
        "\n",
        "    ConvLayers(){}\n",
        "\n",
        "    ConvLayers( int index, convDim_t inDim, kernelDim_t kdims, float a, float b, \n",
        "        cudnnTensorFormat_t t_format, cudnnDataType_t d_type, cudnnConvolutionMode_t c_mode, cudnnActivationMode_t ActMode){\n",
        "\n",
        "        this->POOL = false;\n",
        "        this->inDims = inDim;\n",
        "        this->kernelDims = kdims;\n",
        "        this->layerIndex = index;\n",
        "        this->alph = a;\n",
        "        this->bet = b;\n",
        "        this->TensorFormat = t_format;\n",
        "        this->DataType = d_type;\t\t\t\n",
        "        this->ConvMode = c_mode;\n",
        "        this->ActivationMode = ActMode;\n",
        "    }\n",
        "\n",
        "    void set( int index, convDim_t inDim, kernelDim_t kdims, float a, float b, \n",
        "        cudnnTensorFormat_t t_format, cudnnDataType_t d_type, cudnnConvolutionMode_t c_mode, cudnnActivationMode_t ActMode){\n",
        "\n",
        "        this->POOL = false;\n",
        "        this->inDims = inDim;\n",
        "        this->kernelDims = kdims;\n",
        "        this->layerIndex = index;\n",
        "        this->alph = a;\n",
        "        this->bet = b;\n",
        "        this->TensorFormat = t_format;\n",
        "        this->DataType = d_type;\t\t\t\n",
        "        this->ConvMode = c_mode;\n",
        "        this->ActivationMode = ActMode;\n",
        "    }\n",
        "\n",
        "    ConvLayers(int index, convDim_t inDim, kernelDim_t kdims, poolDim_t pdims, float a, float b, \n",
        "    cudnnTensorFormat_t t_format, cudnnDataType_t d_type, cudnnConvolutionMode_t c_mode, cudnnActivationMode_t ActMode,cudnnPoolingMode_t poolMode){\n",
        "\n",
        "    this->POOL = true;\n",
        "    this->inDims = inDim;\n",
        "    this->kernelDims = kdims;\n",
        "    this->poolDims = pdims;\n",
        "    this->layerIndex = index;\n",
        "    this->alph = a; \n",
        "    this->bet = b;\n",
        "    this->TensorFormat = t_format;\n",
        "    this->DataType = d_type;      \n",
        "    this->ConvMode = c_mode;\n",
        "    this->ActivationMode = ActMode;\n",
        "    this->PoolingMode = poolMode; \n",
        "    }\n",
        "\n",
        "    void set(int index, convDim_t inDim, kernelDim_t kdims, poolDim_t pdims, float a, float b, \n",
        "    cudnnTensorFormat_t t_format, cudnnDataType_t d_type, cudnnConvolutionMode_t c_mode, cudnnActivationMode_t ActMode,cudnnPoolingMode_t poolMode){\n",
        "\n",
        "    this->POOL = true;\n",
        "    this->inDims = inDim;\n",
        "    this->kernelDims = kdims;\n",
        "    this->poolDims = pdims;\n",
        "    this->layerIndex = index;\n",
        "    this->alph = a; \n",
        "    this->bet = b;\n",
        "    this->TensorFormat = t_format;\n",
        "    this->DataType = d_type;      \n",
        "    this->ConvMode = c_mode;\n",
        "    this->ActivationMode = ActMode;\n",
        "    this->PoolingMode = poolMode; \n",
        "    }\n",
        "\n",
        "    ConvLayers(const ConvLayers& old_layer){\n",
        "        this->POOL = old_layer.POOL;\n",
        "        this->inDims = old_layer.inDims;\n",
        "        this->kernelDims = old_layer.kernelDims;\n",
        "        this->poolDims = old_layer.poolDims;\n",
        "        this->layerIndex = old_layer.layerIndex;\n",
        "        this->alph = old_layer.alph; \n",
        "        this->bet = old_layer.bet;\n",
        "        this->TensorFormat = old_layer.TensorFormat;\n",
        "        this->DataType = old_layer.DataType;      \n",
        "        this->ConvMode = old_layer.ConvMode;\n",
        "        this->ActivationMode = old_layer.ActivationMode;\n",
        "        if(old_layer.POOL == true)\n",
        "            this->PoolingMode = old_layer.PoolingMode; \n",
        "    }\n",
        "\n",
        "    void getConvLayerSpecs();\n",
        "\n",
        "    void buildConvLayer(float* inbias, vector< vector<vector< vector <float> > > >& inkernal);\n",
        "\n",
        "    void fwdProp(cudaStream_t stream, cudnnHandle_t CUDNN, float *inputTensor, float* &outputTensor, float* &conv_output, float* &poolTensor, void* &d_workspace);\n",
        "\n",
        "};\n",
        "\n",
        "\n",
        "void ConvLayers::getConvLayerSpecs(){\n",
        "\n",
        "\n",
        "\n",
        "  checkCUDNN(cudnnCreateTensorDescriptor(&input_descriptor));\n",
        "  checkCUDNN(cudnnSetTensor4dDescriptor(input_descriptor,\n",
        "                                          TensorFormat,\n",
        "                                          DataType,\n",
        "                                          inDims.Batch,\n",
        "                                          inDims.Channels, \n",
        "                                          inDims.Height, \n",
        "                                          inDims.Width));\n",
        "\n",
        "   \n",
        "  // --- Build the Kernel which is going to convolve over the input ---//\n",
        "  \n",
        "  \n",
        "  checkCUDNN(cudnnCreateFilterDescriptor(&kernel_descriptor));\n",
        "  checkCUDNN(cudnnSetFilter4dDescriptor(kernel_descriptor,\n",
        "                                        DataType,\n",
        "                                        TensorFormat,\n",
        "                                        kernelDims.kernelSize,\n",
        "                                        inDims.Channels,\n",
        "                                        kernelDims.kernelHeight,\n",
        "                                        kernelDims.kernelWidth));\n",
        "\n",
        "  // --- Build the Convolution descriptor --- //\n",
        "\n",
        "  \n",
        "  checkCUDNN(cudnnCreateConvolutionDescriptor(&convolution_descriptor));\n",
        "  checkCUDNN(cudnnSetConvolution2dDescriptor(convolution_descriptor,\n",
        "                                            kernelDims.padHeight,\n",
        "                                            kernelDims.padWidth,\n",
        "                                            kernelDims.strideHeight,\n",
        "                                            kernelDims.strideWidth,\n",
        "                                            kernelDims.dilationHeight,\n",
        "                                            kernelDims.dilationWidth,\n",
        "                                            ConvMode,\n",
        "                                            DataType));\n",
        "\n",
        "  // --- This function returns the dimensions of the resulting 4D tensor of a 2D convolution,     //\n",
        "  // ---given the convolution descriptor, the input tensor descriptor and the filter descriptor --- //\n",
        "\n",
        "  checkCUDNN(cudnnGetConvolution2dForwardOutputDim(convolution_descriptor,\n",
        "                                                 input_descriptor,\n",
        "                                                 kernel_descriptor,\n",
        "                                                 &convOutDimBatchSize,\n",
        "                                                 &convOutDimChannels,\n",
        "                                                 &convOutDimHeight,\n",
        "                                                 &convOutDimWidth));\n",
        "  \n",
        "  outDims.Height = convOutDimHeight;\n",
        "  outDims.Width = convOutDimWidth;\n",
        "  outDims.Channels = convOutDimChannels;\n",
        "  outDims.Batch = convOutDimBatchSize;\n",
        "  \n",
        "  checkCUDNN(cudnnCreateTensorDescriptor(&bias_descriptor));\n",
        "  checkCUDNN(cudnnSetTensor4dDescriptor(bias_descriptor,\n",
        "                                            TensorFormat,\n",
        "                                            DataType,\n",
        "                                            convOutDimBatchSize,\n",
        "                                           convOutDimChannels,\n",
        "                                           convOutDimHeight,\n",
        "                                           convOutDimWidth));\n",
        "\n",
        "  // ---Build the output Descriptor ---//\n",
        "\n",
        "  \n",
        "  checkCUDNN(cudnnCreateTensorDescriptor(&convOutput_descriptor));\n",
        "  checkCUDNN(cudnnSetTensor4dDescriptor(convOutput_descriptor,\n",
        "                                        TensorFormat,\n",
        "                                        DataType, \n",
        "                                        convOutDimBatchSize,\n",
        "                                        convOutDimChannels,\n",
        "                                        convOutDimHeight,\n",
        "                                        convOutDimWidth));\n",
        "\n",
        "  // -- Size references for next conv layer --- //\n",
        "\n",
        "  \n",
        "  // REMOVED THIS, PUT IT IN FORWARD\n",
        "\n",
        "  \n",
        "  checkCUDNN(cudnnCreateActivationDescriptor(&activation_descriptor));\n",
        "  checkCUDNN(cudnnSetActivationDescriptor(activation_descriptor,\n",
        "                                        ActivationMode,\n",
        "                                        CUDNN_PROPAGATE_NAN,\n",
        "                                        /*relu_coef=*/0));\n",
        "\n",
        "\n",
        "  /*\n",
        "  Do some adjustment if the output dimension of pooling layer is not an integer (which will give an error) \n",
        "  Each dimension h and w of the output images is computed as followed:\n",
        "  outputDim = 1 + (inputDim + 2*padding - windowDim)/poolingStride;\n",
        "  */\n",
        "\n",
        "  // check if the user has asked to create a pooling layer for this conv layer\n",
        "  if(POOL){\n",
        "\n",
        "    if((outDims.Height - poolDims.Height)%2 != 0){\n",
        "      poolDims.Height = (poolDims.Height == 2) ? 3 : 2;\n",
        "    }\n",
        "\n",
        "    if((outDims.Width - poolDims.Width)%2 != 0){\n",
        "      poolDims.Width = (poolDims.Width == 2) ? 3 : 2;\n",
        "    }\n",
        "\n",
        "    \n",
        "    checkCUDNN(cudnnCreatePoolingDescriptor(&pooling_descriptor));\n",
        "    checkCUDNN(cudnnSetPooling2dDescriptor(pooling_descriptor,\n",
        "                                            PoolingMode,\n",
        "                                            CUDNN_NOT_PROPAGATE_NAN,\n",
        "                                            poolDims.Height,\n",
        "                                            poolDims.Width,\n",
        "                                            poolDims.padHeight,\n",
        "                                            poolDims.padWidth,\n",
        "                                            poolDims.strideHeight,\n",
        "                                            poolDims.strideWidth));\n",
        "\n",
        "    checkCUDNN(cudnnGetPooling2dForwardOutputDim(pooling_descriptor,\n",
        "                                              convOutput_descriptor,\n",
        "                                                  &poolOutBatchSize,\n",
        "                                                  &poolOutChannels,\n",
        "                                                  &poolOutHeight,\n",
        "                                                  &poolOutWidth));\n",
        "\n",
        "    \n",
        "    checkCUDNN(cudnnCreateTensorDescriptor(&poolTensor_descriptor));  \n",
        "    checkCUDNN(cudnnSetTensor4dDescriptor(poolTensor_descriptor,\n",
        "                                          TensorFormat,\n",
        "                                          DataType,\n",
        "                                          poolOutBatchSize,\n",
        "                                          poolOutChannels,\n",
        "                                          poolOutHeight,\n",
        "                                          poolOutWidth));\n",
        "\n",
        "    outDims.Batch = poolOutBatchSize;\n",
        "    outDims.Channels = poolOutChannels;\n",
        "    outDims.Height = poolOutHeight;\n",
        "    outDims.Width = poolOutWidth;\n",
        "\n",
        "    }\n",
        "\n",
        "    cout<<\"Output image size \"<<outDims.Batch<<\" X \"<<outDims.Height<<\" X \"<<outDims.Width<<\" X \"<<outDims.Channels<<endl;\n",
        "}\n",
        "\n",
        "void ConvLayers::buildConvLayer(float* inbias, vector< vector<vector< vector <float> > > >& inkernal){\n",
        "\n",
        "    // Initialize bias and kernel tensors here //\n",
        "    // Bias\n",
        "    cudaMallocManaged(&biasTensor, outDims.Channels * outDims.Batch * sizeof(float));\n",
        "    cudaMemcpy(biasTensor,inbias,outDims.Channels * outDims.Batch * sizeof(float),cudaMemcpyHostToDevice);\n",
        "\n",
        "    float hkernel[kernelDims.kernelSize][inDims.Channels][kernelDims.kernelHeight][kernelDims.kernelWidth];\n",
        "\n",
        "    for(int i = 0; i < kernelDims.kernelSize; i++){\n",
        "      for(int j = 0; j < inDims.Channels; j++){\n",
        "        for(int k = 0; k < kernelDims.kernelHeight; k++){\n",
        "          for(int l = 0; l < kernelDims.kernelWidth; l++){\n",
        "            hkernel[i][j][k][l] = inkernal[i][j][k][l]; \n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    cudaMallocManaged(&kernelTensor, kernelDims.kernelSize * kernelDims.kernelHeight * kernelDims.kernelWidth * sizeof(float));\n",
        "    cudaMemcpy(kernelTensor,hkernel,sizeof(hkernel),cudaMemcpyHostToDevice);\n",
        "}\n",
        "\n",
        "\n",
        "void ConvLayers::fwdProp(cudaStream_t stream, cudnnHandle_t CUDNN, float *inputTensor, float* &outputTensor, float* &conv_output, float* &poolTensor, void* &d_workspace)\n",
        "{\n",
        "\n",
        "    /**\n",
        "    float* conv_output{nullptr}; // output of convolution operation\n",
        "    float* poolTensor{nullptr};  // output of pooling layer, if exists\n",
        "    void* d_workspace{nullptr};\n",
        "    size_t workspaceBytes{0};\n",
        "    **/\n",
        "   // --- Determine the Convolution algorithm to be used in CNN layer ---//\n",
        "  checkCUDNN(cudnnGetConvolutionForwardAlgorithm(CUDNN,\n",
        "                                        input_descriptor,\n",
        "                                        kernel_descriptor,\n",
        "                                        convolution_descriptor,\n",
        "                                        convOutput_descriptor,\n",
        "                                        CUDNN_CONVOLUTION_FWD_PREFER_FASTEST,\n",
        "                                        /*memoryLimitInBytes=*/0,\n",
        "                                        &convolution_algorithm));\n",
        "\n",
        "    size_t workspaceBytes{0};\n",
        "    // --- Set up the memory required for the convolution --- //\n",
        "    checkCUDNN(cudnnGetConvolutionForwardWorkspaceSize(CUDNN,\n",
        "                                                     input_descriptor,\n",
        "                                                     kernel_descriptor,\n",
        "                                                     convolution_descriptor,\n",
        "                                                     convOutput_descriptor,\n",
        "                                                     convolution_algorithm,\n",
        "                                                     &workspaceBytes));\n",
        "    // --- Allocate Memory in the GPU for layer operation --- //    \n",
        "    cudaMallocManaged(&d_workspace, workspaceBytes);\n",
        "\n",
        "    int convout_bytes = convOutDimBatchSize * convOutDimChannels * convOutDimHeight * convOutDimWidth * sizeof(float);    \n",
        "    // memory required for storing output of the conv operation (after adding bias)\n",
        "    cudaMallocManaged(&conv_output, convout_bytes);\n",
        "    cudaMemsetAsync(conv_output, 0, convout_bytes, stream);\n",
        "\n",
        "    // set up memory for pool tensor if pool is true\n",
        "    if(POOL){\n",
        "      int poolSize =  outDims.Batch * outDims.Channels * outDims.Height * outDims.Width * sizeof(float);\n",
        "      cudaMallocManaged(&poolTensor, poolSize); \n",
        "      cudaMemsetAsync(poolTensor, 0, poolSize, stream);\n",
        "    }\n",
        "\n",
        "    checkCUDNN(cudnnConvolutionForward(CUDNN,\n",
        "                                    &alph,\n",
        "                                    input_descriptor,\n",
        "                                    inputTensor,\n",
        "                                    kernel_descriptor,\n",
        "                                    kernelTensor,\n",
        "                                    convolution_descriptor,\n",
        "                                    convolution_algorithm,\n",
        "                                    d_workspace,\n",
        "                                    workspaceBytes,\n",
        "                                    &bet,\n",
        "                                    convOutput_descriptor,\n",
        "                                    conv_output));\n",
        "\n",
        "    checkCUDNN(cudnnAddTensor(CUDNN, &alph, bias_descriptor,\n",
        "                                biasTensor ,&bet, convOutput_descriptor, conv_output));\n",
        "\n",
        "    checkCUDNN(cudnnActivationForward(CUDNN,\n",
        "                                    activation_descriptor,\n",
        "                                    &alph,\n",
        "                                    convOutput_descriptor,\n",
        "                                    conv_output,\n",
        "                                    &bet,\n",
        "                                    convOutput_descriptor,\n",
        "                                    conv_output));\n",
        "\n",
        "    if(POOL){\n",
        "        checkCUDNN(cudnnPoolingForward(CUDNN,\n",
        "                                    pooling_descriptor,\n",
        "                                    &alph,\n",
        "                                    convOutput_descriptor,\n",
        "                                    conv_output,\n",
        "                                    &bet,\n",
        "                                    poolTensor_descriptor,\n",
        "                                    poolTensor));\n",
        "\n",
        "        cudaMallocManaged(&outputTensor,sizeof(poolTensor));\n",
        "        cudaMemcpyAsync(outputTensor,poolTensor,sizeof(poolTensor),cudaMemcpyDeviceToDevice, stream);\n",
        "    }\n",
        "    else{\n",
        "\n",
        "        cudaMallocManaged(&outputTensor,sizeof(conv_output));\n",
        "        cudaMemcpyAsync(outputTensor,conv_output,sizeof(conv_output),cudaMemcpyDeviceToDevice, stream);  \t\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "__global__\n",
        "void addBiasFC(int dim1,int dim2, float* bias, float* res){\n",
        "\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  if(idx < dim1*dim2){\n",
        "    res[idx] += bias[idx];\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "class FCLayers : public ConvLayers{\n",
        "\n",
        "  public:\n",
        "    // cublasHandle_t CUBLAS;\n",
        "    int inDims;\n",
        "    int outDims;\n",
        "    int batch;\n",
        "    float *weights{nullptr};\n",
        "    float *dcost{nullptr};\n",
        "    // float *p_act{nullptr};\n",
        "    float *nabla_w{nullptr};\n",
        "    float *nabla_b{nullptr};\n",
        "    float* ones{nullptr};\n",
        "    float* d_intermediate{nullptr};\n",
        "    float* dReLU_tensor{nullptr};\n",
        "    \n",
        "\n",
        "\n",
        "    bool last;\n",
        "    float* outputTensor{nullptr};\n",
        "    cudnnTensorDescriptor_t outputTensor_descriptor;\n",
        "    FCLayers(){}\n",
        "\n",
        "    FCLayers(int inDims_, int batch,int outDims_, float alpha, float beta, cudnnActivationMode_t ActivationMode_, \n",
        "      cudnnTensorFormat_t t_format, cudnnDataType_t d_type){\n",
        "\n",
        "      this->last = false;\n",
        "      this->inDims = inDims_;\n",
        "      this->batch = batch;\n",
        "      this->outDims = outDims_;\n",
        "      this->ActivationMode = ActivationMode_;      \n",
        "      this->alph = alpha;\n",
        "      this->bet = beta;\n",
        "      this->DataType = d_type;\n",
        "      this->TensorFormat = t_format;\n",
        "    }\n",
        "\n",
        "    void set(int inDims_, int batch,int outDims_, float alpha, float beta, cudnnActivationMode_t ActivationMode_, \n",
        "      cudnnTensorFormat_t t_format, cudnnDataType_t d_type){\n",
        "\n",
        "      this->last = false;\n",
        "      this->inDims = inDims_;\n",
        "      this->batch = batch;\n",
        "      this->outDims = outDims_;\n",
        "      this->ActivationMode = ActivationMode_;      \n",
        "      this->alph = alpha;\n",
        "      this->bet = beta;\n",
        "      this->DataType = d_type;\n",
        "      this->TensorFormat = t_format;\n",
        "    }\n",
        "\n",
        "    void getFCLayerSpecs();\n",
        "    void buildFCLayer(float* inbias, vector< vector <float> >& inkernal);\n",
        "    void fwdProp(cublasHandle_t CUBLAS, cudnnHandle_t CUDNN, cudaStream_t stream, float* inputTensor, float* &outputTensor);\n",
        "\n",
        "  private:\n",
        "    int numBlocks;\n",
        "    int numThreads;\n",
        "};\n",
        "\n",
        "\n",
        "\n",
        "void FCLayers::getFCLayerSpecs(){\n",
        "\n",
        "    checkCUDNN(cudnnCreateActivationDescriptor(&activation_descriptor));\n",
        "    checkCUDNN(cudnnSetActivationDescriptor(activation_descriptor,\n",
        "                                    ActivationMode,\n",
        "                                    CUDNN_PROPAGATE_NAN,\n",
        "                                    0));\n",
        "\n",
        "    checkCUDNN(cudnnCreateTensorDescriptor(&outputTensor_descriptor));\n",
        "    checkCUDNN(cudnnSetTensor4dDescriptor(outputTensor_descriptor,\n",
        "                                      TensorFormat,\n",
        "                                      DataType,\n",
        "                                      batch, outDims, 1, 1));\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "void FCLayers::buildFCLayer(float* inbias, vector< vector <float> >& inkernal){\n",
        "\n",
        "  // Initialization of weight matrix\n",
        "\n",
        "  \n",
        "  float *hweights;\n",
        "  hweights = (float*)malloc(sizeof(float)*inDims*outDims);\n",
        "  for(int i = 0; i < outDims; i++){\n",
        "          for(int j=0;j<inDims;j++)\n",
        "   hweights[i*outDims + j] = inkernal[i][j];\n",
        "  }\n",
        "\n",
        "  cudaMallocManaged(&weights, inDims*outDims*sizeof(float));\n",
        "  cudaMemcpy(weights, hweights, inDims * outDims * sizeof(float),cudaMemcpyHostToDevice);\n",
        "\n",
        "  free(hweights);\n",
        "  // initialization of bias vector\n",
        "  cudaMallocManaged(&biasTensor,outDims*batch*sizeof(float));\n",
        "  cudaMemcpy(biasTensor,inbias,outDims*batch*sizeof(float),cudaMemcpyHostToDevice);\n",
        "    \n",
        "\n",
        "  \n",
        "  // cudaMemset(outputTensor,0,outDims*batch*sizeof(float)); ----- If not using cudaMemset(),\n",
        "  // ensure that while performing any operation on it, its multiplicatio coeff is 0, like in cublasSgemm() below, bet is 0 for the same reason\n",
        "\n",
        "  // Decide the number of threads and blocks based on the size of the output of the FC layer (before adding the bias) for addBiasFC kernel\n",
        "  if(batch*outDims <= MAX_THREADS_PER_BLOCK){\n",
        "    numThreads = batch*outDims;\n",
        "    numBlocks = 1;\n",
        "  }\n",
        "  else{\n",
        "    numBlocks = roundUp(batch*outDims,MAX_THREADS_PER_BLOCK);\n",
        "    numThreads = MAX_THREADS_PER_BLOCK;\n",
        "  }\n",
        "    cout<<\"Output image size \"<<outDims<<endl;\n",
        "}\n",
        "\n",
        "\n",
        "void FCLayers::fwdProp(cublasHandle_t CUBLAS, cudnnHandle_t CUDNN, cudaStream_t stream, float* inputTensor, float* &outputTensor){\n",
        "\n",
        "  cudaMallocManaged(&outputTensor,outDims*batch*sizeof(float));\n",
        "\n",
        "  cublasSgemm(CUBLAS,\n",
        "        CUBLAS_OP_T,CUBLAS_OP_N,\n",
        "        outDims, batch, inDims,\n",
        "        &alph,\n",
        "        weights,inDims,\n",
        "        inputTensor,inDims,\n",
        "        &bet,\n",
        "        outputTensor,outDims);\n",
        "  \n",
        "  addBiasFC<<<numBlocks,numThreads,0,stream>>>(outDims,batch,biasTensor,outputTensor);\n",
        "  \n",
        "  checkCUDNN(cudnnActivationForward(CUDNN,\n",
        "                  activation_descriptor,\n",
        "                  &alph,\n",
        "                  outputTensor_descriptor,\n",
        "                  outputTensor,\n",
        "                  &bet,\n",
        "                  outputTensor_descriptor,\n",
        "                  outputTensor));\n",
        "  \n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "ConvLayers convlayer1;\n",
        "ConvLayers convlayer2;\n",
        "ConvLayers convlayer3;\n",
        "ConvLayers convlayer4;\n",
        "ConvLayers convlayer5;\n",
        "FCLayers fclayer1;\n",
        "FCLayers fclayer2;\n",
        "FCLayers fclayer3;\n",
        "\n",
        "class Kernel{\n",
        "\n",
        "  public :\n",
        "    cudaStream_t stream;\n",
        "    cublasHandle_t cublas;\n",
        "    cudnnHandle_t cudnn;\n",
        "    int id;\n",
        "    int layer;\n",
        "    double tbRatio;\n",
        "    float ****fourDInput;\n",
        "    float ****fourDOutput;\n",
        "    float ** twoDInput;\n",
        "    float ** twoDOutput;\n",
        "    double execTime;\n",
        "    double compTime;\n",
        "    \n",
        "\n",
        "    Kernel(){}\n",
        "    \n",
        "    Kernel(cudaStream_t stream,\n",
        "        cublasHandle_t cublas,\n",
        "        cudnnHandle_t cudnn,\n",
        "        int layerIndex,\n",
        "        float ****fourDInput,\n",
        "        double startTime,\n",
        "        double deadTime,\n",
        "        double tbratio,\n",
        "        int id){\n",
        "            this->stream = stream;\n",
        "            this->cublas = cublas;\n",
        "            this->cudnn = cudnn;\n",
        "            this->layer = layerIndex;\n",
        "            this->fourDInput = fourDInput;\n",
        "            this->execTime = deadTime - startTime;\n",
        "            this->compTime = LAYERS*CONST_TIME;\n",
        "            this->tbRatio = tbratio;\n",
        "            this->id = id;\n",
        "        }\n",
        "    \n",
        "    Kernel(cudaStream_t stream,\n",
        "        cublasHandle_t cublas,\n",
        "        cudnnHandle_t cudnn,\n",
        "        int layerIndex,\n",
        "        float **twoDInput,\n",
        "        double startTime,\n",
        "        double deadTime,\n",
        "        double tbratio,\n",
        "        int id){\n",
        "            this->stream = stream;\n",
        "            this->cublas = cublas;\n",
        "            this->cudnn = cudnn;\n",
        "            this->layer = layerIndex;\n",
        "            this->twoDInput = twoDInput;\n",
        "            this->execTime = deadTime - startTime;\n",
        "            this->compTime = LAYERS*CONST_TIME;\n",
        "            this->tbRatio = tbratio;\n",
        "            this->id =id;\n",
        "        }\n",
        "    \n",
        "    void update()\n",
        "    {\n",
        "        layer++;\n",
        "        compTime -= CONST_TIME;\n",
        "        // TODO update tbratio properly\n",
        "        if(layer >= 7){\n",
        "            tbRatio = 0.2;\n",
        "            twoDInput = twoDOutput;\n",
        "        }\n",
        "        else{\n",
        "            fourDInput = fourDOutput;\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "};\n",
        "\n",
        "void execute(vector<Kernel> C);\n",
        "\n",
        "\n",
        "typedef pair<int, Kernel> Node;\n",
        "\n",
        "struct cmp\n",
        "{\n",
        "\tbool operator()(const Node &A,const Node &B) const\n",
        "\t{\n",
        "\t\tif(A.first != B.first)\n",
        "            return A.first < B.first;\n",
        "        else\n",
        "            return A.second.execTime - A.second.compTime < B.second.execTime - B.second.compTime;\n",
        "\t}\n",
        "};\n",
        "\n",
        "typedef set<Node, cmp> SET;\n",
        "\n",
        "bool compareKernel(Kernel k1, Kernel k2){\n",
        "    return k1.compTime < k2.compTime;\n",
        "}\n",
        "\n",
        "int getSlackTime(int compTime, int execTime){\n",
        "    return execTime - compTime;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "vector<Kernel> G;\n",
        "\n",
        "SET enqueue(SET& Queue, Kernel k){\n",
        "    SET newQueue;\n",
        "    newQueue.insert({getSlackTime(k.compTime, k.execTime), k});\n",
        "    for(SET::iterator it=Queue.begin();it!=Queue.end();it++){\n",
        "        Kernel temp_kernel = it->second;\n",
        "        temp_kernel.execTime -= 1;\n",
        "        newQueue.insert({getSlackTime(temp_kernel.compTime,temp_kernel.execTime),temp_kernel});\n",
        "    }\n",
        "    Queue.clear();\n",
        "    return newQueue;\n",
        "}\n",
        "\n",
        "\n",
        "SET dequeue(SET& Queue){\n",
        "    SET newQueue;\n",
        "    \n",
        "    cout<<\"Printing Queue\"<<endl;\n",
        "    for(SET:: iterator it=Queue.begin();it!=Queue.end();it++){\n",
        "        Kernel temp_kernel = it->second;\n",
        "        cout<<\"ID: \"<<temp_kernel.id<<endl;\n",
        "        temp_kernel.execTime -= CONST_TIME;\n",
        "        newQueue.insert({getSlackTime(temp_kernel.compTime,temp_kernel.execTime),temp_kernel});\n",
        "    }\n",
        "    Queue.clear();\n",
        "    vector<Kernel> C;\n",
        "    if(G.size()>0){\n",
        "        execute(G);\n",
        "        cout<<\"Execution size \"<<G.size()<<endl;\n",
        "    }\n",
        "        \n",
        "    for(int i=0;i<G.size();i++){\n",
        "        cout<<\"EXEC Layer: \"<<G[i].layer<<endl;\n",
        "        cout<<\"Priority:\"<<G[i].execTime - G[i].compTime<<endl;\n",
        "        if(G[i].layer != LAYERS){\n",
        "            G[i].update();\n",
        "            newQueue = enqueue(newQueue, G[i]);\n",
        "        }\n",
        "    }\n",
        "    G.clear();\n",
        "    Kernel temp = (*(newQueue.begin())).second;\n",
        "    newQueue.erase(newQueue.begin());\n",
        "    G.push_back(temp);\n",
        "    double total = 0;\n",
        "    total += temp.tbRatio;\n",
        "    if(total < 1){\n",
        "        for(SET::iterator it = newQueue.begin();it!=newQueue.end();){\n",
        "            if((*it).second.tbRatio<1){\n",
        "                Kernel temp2 = (*it).second;\n",
        "                total += temp2.tbRatio;\n",
        "                it = newQueue.erase(it);\n",
        "                G.push_back(temp2);\n",
        "            }\n",
        "            else\n",
        "            {\n",
        "                it++;\n",
        "            }\n",
        "            cout<<\"Total TB \"<<total<<endl;\n",
        "            if(total > 1)\n",
        "                break;\n",
        "        }\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        cout<<\"Single kernel execution\"<<endl;\n",
        "        execute(G);\n",
        "        for(int i=0;i<G.size();i++){\n",
        "            cout<<\"Priority:\"<<G[i].execTime - G[i].compTime<<endl;\n",
        "            if(G[i].layer != LAYERS){\n",
        "                G[i].update();\n",
        "                newQueue = enqueue(newQueue, G[i]);\n",
        "            }\n",
        "        }\n",
        "        G.clear();\n",
        "\n",
        "        return newQueue;\n",
        "    }\n",
        "    //lookahead to be implemented\n",
        "\n",
        "\n",
        "    execute(G);\n",
        "    cout<<\"Execution size: \"<<G.size()<<endl;\n",
        "    for(int i=0;i<G.size();i++){\n",
        "        cout<<\"Priority:\"<<G[i].execTime - G[i].compTime<<endl;\n",
        "        if(G[i].layer != LAYERS){\n",
        "            G[i].update();\n",
        "            newQueue = enqueue(newQueue, G[i]);\n",
        "        }\n",
        "    }\n",
        "    G.clear();\n",
        "\n",
        "    return newQueue;   \n",
        "}\n",
        "\n",
        "\n",
        "void call_fc1(FCLayers &fclayer1, cudaStream_t stream, cudnnHandle_t cudnn, cublasHandle_t cublas, float ****& input, float** &output)\n",
        "{\n",
        "    float *input_layer1;\n",
        "    convDim_t InputDims = setConvSpecs(7, 7, 256, BATCH_SIZE);\n",
        "    cudaMallocManaged(&input_layer1, InputDims.Height * InputDims.Width * InputDims.Channels * InputDims.Batch * sizeof(float));\n",
        "    cudaMemcpyAsync(input_layer1, input, InputDims.Height * InputDims.Width * InputDims.Channels * InputDims.Batch * sizeof(float), cudaMemcpyHostToDevice, stream);\n",
        "\n",
        "    float *outputTensor;\n",
        "    \n",
        "    fclayer1.fwdProp(cublas, cudnn, stream, input_layer1, outputTensor);\n",
        "\n",
        "    int out_size =  4096*BATCH_SIZE;\n",
        "\n",
        "    output = (float **) malloc(BATCH_SIZE*sizeof(float*));\n",
        "    for(int i=0;i<BATCH_SIZE;++i) output[i] = (float *) malloc(4096*sizeof(float));\n",
        "    for(int i=0;i<BATCH_SIZE;++i){\n",
        "        for(int j=0;j<4096;++j){\n",
        "            output[i][j] = 0;\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    cudaMemcpyAsync(output, outputTensor, out_size*sizeof(float), cudaMemcpyDeviceToHost, stream);\n",
        "    cout << \"FC1 DONE\" << endl;\n",
        "}\n",
        "\n",
        "\n",
        "void call_fc2(FCLayers &fclayer2, cudaStream_t stream, cudnnHandle_t cudnn, cublasHandle_t cublas, float **& input, float** &output)\n",
        "{\n",
        "    float *input_layer2;\n",
        "    int InputDims = 4096;\n",
        "    cudaMallocManaged(&input_layer2, InputDims * BATCH_SIZE * sizeof(float));\n",
        "    cudaMemcpyAsync(input_layer2, input, InputDims * BATCH_SIZE * sizeof(float), cudaMemcpyHostToDevice, stream);\n",
        "\n",
        "    float * outputTensor; \n",
        "    fclayer2.fwdProp(cublas, cudnn, stream, input_layer2, outputTensor);\n",
        "\n",
        "    int out_size =  4096*BATCH_SIZE;\n",
        " \n",
        "    output = (float **) malloc(BATCH_SIZE*sizeof(float*));\n",
        "    for(int i=0;i<BATCH_SIZE;++i) output[i] = (float *) malloc(4096*sizeof(float));\n",
        "    for(int i=0;i<BATCH_SIZE;++i){\n",
        "        for(int j=0;j<4096;++j){\n",
        "            output[i][j] = 0;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    cudaMemcpyAsync(output, outputTensor, out_size*sizeof(float), cudaMemcpyDeviceToHost, stream);\n",
        "    cout << \"FC2 DONE\" << endl;\n",
        "}\n",
        "\n",
        "\n",
        "void call_fc3(FCLayers &fclayer3, cudaStream_t stream, cudnnHandle_t cudnn, cublasHandle_t cublas, float **& input, float** &output)\n",
        "{\n",
        "\n",
        "    float *input_layer3;\n",
        "    int InputDims = 4096;\n",
        "    cudaMallocManaged(&input_layer3, InputDims * BATCH_SIZE * sizeof(float));\n",
        "    cudaMemcpyAsync(input_layer3, input, InputDims * BATCH_SIZE * sizeof(float), cudaMemcpyHostToDevice, stream);\n",
        "\n",
        "    float * outputTensor;\n",
        "    fclayer3.fwdProp(cublas, cudnn, stream, input_layer3, outputTensor);\n",
        "\n",
        "    int out_size =  1000*BATCH_SIZE;\n",
        "    cout<<\"HI2\"<<endl;\n",
        "    output = (float **) malloc(BATCH_SIZE*sizeof(float*));\n",
        "    cout<<\"HI3\"<<endl;\n",
        "    for(int i=0;i<BATCH_SIZE;++i) output[i] = (float *) malloc(1000*sizeof(float));\n",
        "    for(int i=0;i<BATCH_SIZE;++i){\n",
        "        for(int j=0;j<1000;++j){\n",
        "            output[i][j] = 0;\n",
        "        }\n",
        "    }\n",
        "    cout<<\"HI4\"<<endl;\n",
        "    cudaMemcpyAsync(output, outputTensor, out_size*sizeof(float), cudaMemcpyDeviceToHost, stream);\n",
        "    cout << \"FC3 DONE\" << endl;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "void fill_vec(vector<vector<vector<vector<float> > > > &ker, float bia[], string file_loc, int x, int y, int z, int w){\n",
        "  \n",
        "  std::ifstream File(file_loc);\n",
        "  std::string Str;\n",
        "  int b_c=0,i=0,j=0,k=0,l=0,count=0,length=y*z*w+1;\n",
        "  while (std::getline(File, Str))\n",
        "  {\n",
        "      istringstream ss(Str);\n",
        "      string word;\n",
        "      while (ss >> word) \n",
        "      {\n",
        "          if(count%length==0){\n",
        "              bia[b_c++]=stof(word);\n",
        "          }\n",
        "          else{\n",
        "              ker[i][j][k][l++]=stof(word);\n",
        "              if(l==w)k++,l=0;\n",
        "              if(k==z)j++,k=0;\n",
        "              if(j==y)i++,j=0;\n",
        "          }\n",
        "          count++;\n",
        "          //cout << word << \"\\n\";\n",
        "      }\n",
        "  }\n",
        "  //cout<<count<<'\\n';\n",
        "  \n",
        "  File.close();\n",
        "}\n",
        "\n",
        "void fill_vec(vector<vector<float> > &ker, float bia[], string file_loc, int x, int y){\n",
        "  \n",
        "  std::ifstream File(file_loc);\n",
        "  std::string Str;\n",
        "  int b_c=0,i=0,j=0,count=0,length=y+1;\n",
        "  while (std::getline(File, Str))\n",
        "  {\n",
        "      istringstream ss(Str);\n",
        "      string word;\n",
        "      while (ss >> word) \n",
        "      {\n",
        "          if(count%length==0){\n",
        "              bia[b_c++]=stof(word);\n",
        "          }\n",
        "          else{\n",
        "              ker[i][j++]=stof(word);\n",
        "              if(j==y)i++,j=0;\n",
        "          }\n",
        "          count++;\n",
        "          //cout << word << \"\\n\";\n",
        "      }\n",
        "  }\n",
        "  //cout<<count<<'\\n';\n",
        "  \n",
        "  File.close();\n",
        "}\n",
        "\n",
        "void fill_vec_special(vector<vector<float> > &ker, float bia[], string file_loc, int x, int y){\n",
        "  \n",
        "  std::ifstream File(file_loc);\n",
        "  std::string Str;\n",
        "  int b_c=0,i=0,j=0,j_fill=0,count=0,length=y+1;\n",
        "  while (std::getline(File, Str))\n",
        "  {\n",
        "      istringstream ss(Str);\n",
        "      string word;\n",
        "      while (ss >> word) \n",
        "      {\n",
        "          if(count%length==0){\n",
        "              bia[b_c++]=stof(word);\n",
        "          }\n",
        "          else{\n",
        "              ker[i][j_fill++]=stof(word);\n",
        "              j++;\n",
        "              if(j%36==0){\n",
        "                  j_fill+=13;\n",
        "              }\n",
        "              if(j_fill==y)i++,j_fill=0,j=0;\n",
        "          }\n",
        "          count++;\n",
        "          //cout << word << \"\\n\";\n",
        "      }\n",
        "  }\n",
        "  //cout<<count<<'\\n';\n",
        "  \n",
        "  File.close();\n",
        "}\n",
        "\n",
        "float **** MemAlloc(int batch, int channel, int height, int width){\n",
        "    float ****output_feature;\n",
        "    output_feature = (float****)malloc(batch*sizeof(float***));\n",
        "\n",
        "    for(int i=0;i<batch;++i)\n",
        "    {\n",
        "        output_feature[i] = (float***)malloc(channel*sizeof(float**));\n",
        "        for(int j=0;j<channel;++j)\n",
        "        {\n",
        "            output_feature[i][j] = (float**)malloc(height*sizeof(float*));\n",
        "            for(int k=0;k<height;++k)\n",
        "            {\n",
        "                output_feature[i][j][k] = (float*)malloc(width*sizeof(float));\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for(int i=0;i<batch;++i)\n",
        "    {\n",
        "        for(int j=0;j<channel;++j)\n",
        "        {\n",
        "            for(int k=0;k<height;++k)\n",
        "            {\n",
        "                for(int l=0;l<width;++l)\n",
        "                {\n",
        "                    output_feature[i][j][k][l]=0;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return output_feature;\n",
        "}\n",
        "\n",
        "void call_conv1(ConvLayers &convlayer1, cudaStream_t stream, cudnnHandle_t CUDNN, float**** &input, float**** &output)\n",
        "{\n",
        "    float *input_layer1;\n",
        "    convDim_t firstLayerInputDims = setConvSpecs(227, 227, 3, BATCH_SIZE);\n",
        "    cudaMallocManaged(&input_layer1,firstLayerInputDims.Height * firstLayerInputDims.Width * firstLayerInputDims.Channels * firstLayerInputDims.Batch * sizeof(float));\n",
        "    cudaMemcpyAsync(input_layer1,input,227*227*3* sizeof(float),cudaMemcpyHostToDevice, stream);\n",
        "    float *outputTensor, *conv_output, *poolTensor;\n",
        "    void *d_workspace;\n",
        "\n",
        "    convlayer1.fwdProp(stream, CUDNN, input_layer1, outputTensor, conv_output, poolTensor, d_workspace);\n",
        "\n",
        "    int out_size =  convlayer1.outDims.Height*convlayer1.outDims.Width*convlayer1.outDims.Channels*convlayer1.outDims.Batch;\n",
        "    output = MemAlloc(convlayer1.outDims.Batch, convlayer1.outDims.Channels, convlayer1.outDims.Height, convlayer1.outDims.Width);\n",
        "    cudaMemcpyAsync(output, outputTensor, out_size*sizeof(float), cudaMemcpyDeviceToHost, stream);\n",
        "    cout << \"CONV1 DONE\" << endl;\n",
        "}\n",
        "\n",
        "void call_conv2(ConvLayers &convlayer2, cudaStream_t stream, cudnnHandle_t CUDNN, float ****& input, float**** &output)\n",
        "{\n",
        "    float *input_layer2;\n",
        "    convDim_t InputDims = setConvSpecs(28, 28, 64, BATCH_SIZE);\n",
        "    cudaMallocManaged(&input_layer2,InputDims.Height * InputDims.Width * InputDims.Channels * InputDims.Batch * sizeof(float));\n",
        "    cudaMemcpyAsync(input_layer2,input,InputDims.Height * InputDims.Width * InputDims.Channels * InputDims.Batch * sizeof(float), cudaMemcpyHostToDevice, stream);\n",
        "    float *outputTensor, *conv_output, *poolTensor;\n",
        "    void *d_workspace;\n",
        "\n",
        "    convlayer2.fwdProp(stream, CUDNN, input_layer2, outputTensor, conv_output, poolTensor, d_workspace);\n",
        "\n",
        "    int out_size =  convlayer2.outDims.Height*convlayer2.outDims.Width*convlayer2.outDims.Channels*convlayer2.outDims.Batch;\n",
        "    output = MemAlloc(convlayer2.outDims.Batch, convlayer2.outDims.Channels, convlayer2.outDims.Height, convlayer2.outDims.Width);\n",
        "\n",
        "    cudaMemcpyAsync(output, outputTensor, out_size*sizeof(float), cudaMemcpyDeviceToHost, stream);\n",
        "    cout << \"CONV2 DONE\" << endl;\n",
        "}\n",
        "\n",
        "void call_conv3(ConvLayers &convlayer3, cudaStream_t stream, cudnnHandle_t CUDNN, float ****& input, float**** &output)\n",
        "{\n",
        "    float *input_layer3;\n",
        "    convDim_t InputDims = setConvSpecs(14, 14, 192, BATCH_SIZE);\n",
        "    cudaMallocManaged(&input_layer3,InputDims.Height * InputDims.Width * InputDims.Channels * InputDims.Batch * sizeof(float));\n",
        "    cudaMemcpyAsync(input_layer3,input,InputDims.Height * InputDims.Width * InputDims.Channels * InputDims.Batch * sizeof(float), cudaMemcpyHostToDevice, stream);\n",
        "    float *outputTensor, *conv_output, *poolTensor;\n",
        "    void *d_workspace;\n",
        "\n",
        "    convlayer3.fwdProp(stream, CUDNN, input_layer3, outputTensor, conv_output, poolTensor, d_workspace);\n",
        "\n",
        "    int out_size =  convlayer3.outDims.Height*convlayer3.outDims.Width*convlayer3.outDims.Channels*convlayer3.outDims.Batch;\n",
        "    output = MemAlloc(convlayer3.outDims.Batch, convlayer3.outDims.Channels, convlayer3.outDims.Height, convlayer3.outDims.Width);\n",
        "\n",
        "    cudaMemcpyAsync(output, outputTensor, out_size*sizeof(float), cudaMemcpyDeviceToHost, stream);\n",
        "    cout << \"CONV3 DONE\" << endl;\n",
        "}\n",
        "\n",
        "void call_conv4(ConvLayers &convlayer4, cudaStream_t stream, cudnnHandle_t CUDNN, float ****& input, float**** &output)\n",
        "{\n",
        "    float *input_layer4;\n",
        "    convDim_t InputDims = setConvSpecs(14, 14, 384, BATCH_SIZE);\n",
        "    cudaMallocManaged(&input_layer4,InputDims.Height * InputDims.Width * InputDims.Channels * InputDims.Batch * sizeof(float));\n",
        "    cudaMemcpyAsync(input_layer4,input,InputDims.Height * InputDims.Width * InputDims.Channels * InputDims.Batch * sizeof(float), cudaMemcpyHostToDevice, stream);\n",
        "    float *outputTensor, *conv_output, *poolTensor;\n",
        "    void *d_workspace;\n",
        "\n",
        "    convlayer4.fwdProp(stream, CUDNN, input_layer4, outputTensor, conv_output, poolTensor, d_workspace);\n",
        "\n",
        "    int out_size =  convlayer4.outDims.Height*convlayer4.outDims.Width*convlayer4.outDims.Channels*convlayer4.outDims.Batch;\n",
        "    output = MemAlloc(convlayer4.outDims.Batch, convlayer4.outDims.Channels, convlayer4.outDims.Height, convlayer4.outDims.Width);\n",
        "\n",
        "    cudaMemcpyAsync(output, outputTensor, out_size*sizeof(float), cudaMemcpyDeviceToHost, stream);\n",
        "    cout << \"CONV4 DONE\" << endl;\n",
        "}\n",
        "\n",
        "void call_conv5(ConvLayers &convlayer5, cudaStream_t stream, cudnnHandle_t CUDNN, float ****& input, float**** &output)\n",
        "{\n",
        "    float *input_layer5;\n",
        "    convDim_t InputDims = setConvSpecs(28, 28, 64, BATCH_SIZE);\n",
        "    cudaMallocManaged(&input_layer5,InputDims.Height * InputDims.Width * InputDims.Channels * InputDims.Batch * sizeof(float));\n",
        "    cudaMemcpyAsync(input_layer5,input,InputDims.Height * InputDims.Width * InputDims.Channels * InputDims.Batch * sizeof(float), cudaMemcpyHostToDevice, stream);\n",
        "    float *outputTensor, *conv_output, *poolTensor;\n",
        "    void *d_workspace;\n",
        "\n",
        "    convlayer5.fwdProp(stream, CUDNN, input_layer5, outputTensor, conv_output, poolTensor, d_workspace);\n",
        "\n",
        "    int out_size =  convlayer5.outDims.Height*convlayer5.outDims.Width*convlayer5.outDims.Channels*convlayer5.outDims.Batch;\n",
        "    output = MemAlloc(convlayer5.outDims.Batch, convlayer5.outDims.Channels, convlayer5.outDims.Height, convlayer5.outDims.Width);\n",
        "\n",
        "    cudaMemcpyAsync(output, outputTensor, out_size*sizeof(float), cudaMemcpyDeviceToHost, stream);\n",
        "    cout << \"CONV5 DONE\" << endl;\n",
        "}\n",
        "\n",
        "//  execute all the kernels in the given vector\n",
        "void execute(vector<Kernel> C){\n",
        "    for(int i=0;i<C.size();i++){\n",
        "        Kernel K = C[i];\n",
        "        if(K.layer == 1)\n",
        "            call_conv1(convlayer1, K.stream, K.cudnn, K.fourDInput, K.fourDOutput);\n",
        "        else if(K.layer == 2)\n",
        "            call_conv2(convlayer2, K.stream, K.cudnn, K.fourDInput, K.fourDOutput);\n",
        "        else if(K.layer == 3)\n",
        "            call_conv3(convlayer3, K.stream, K.cudnn, K.fourDInput, K.fourDOutput);\n",
        "        else if(K.layer == 4)\n",
        "            call_conv4(convlayer4, K.stream, K.cudnn, K.fourDInput, K.fourDOutput);\n",
        "        else if(K.layer == 5)\n",
        "            call_conv5(convlayer5, K.stream, K.cudnn, K.fourDInput, K.fourDOutput);\n",
        "        else if(K.layer == 6)\n",
        "            call_fc1(fclayer1, K.stream, K.cudnn, K.cublas, K.fourDInput, K.twoDOutput);\n",
        "        else if(K.layer == 7)\n",
        "            call_fc2(fclayer2, K.stream, K.cudnn, K.cublas, K.twoDInput, K.twoDOutput);       \n",
        "        else\n",
        "            call_fc3(fclayer3, K.stream, K.cudnn, K.cublas, K.twoDInput, K.twoDOutput);\n",
        "    }\n",
        "    cudaDeviceSynchronize();\n",
        "}\n",
        "\n",
        "\n",
        "void FreeMem(float ****output_feature, int batch, int channel, int height, int width)\n",
        "{  \n",
        "    \n",
        "    for(int i=0;i<batch;++i)\n",
        "    {\n",
        "        for(int j=0;j<channel;++j)\n",
        "        {\n",
        "            for(int k=0;k<height;++k)\n",
        "            {\n",
        "                free(output_feature[i][j][k]);\n",
        "            }\n",
        "            free(output_feature[i][j]);\n",
        "        }\n",
        "        free(output_feature[i]);\n",
        "    }\n",
        "}\n",
        "\n",
        "//channel, height, width\n",
        "\tint main(){\n",
        "    float ****img1 = MemAlloc(BATCH_SIZE, 3, 227, 227);\n",
        "    for(int i=0;i<BATCH_SIZE;++i){\n",
        "        for(int j=0; j<3;++j){\n",
        "            for(int k=0;k<227;++k){\n",
        "                for(int l=0;l<227;++l){\n",
        "                    img1[i][j][k][l]=1;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    float ****img2 = MemAlloc(BATCH_SIZE, 3, 227, 227);\n",
        "    for(int i=0;i<BATCH_SIZE;++i){\n",
        "        for(int j=0; j<3;++j){\n",
        "            for(int k=0;k<227;++k){\n",
        "                for(int l=0;l<227;++l){\n",
        "                    img2[i][j][k][l]=1;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\t//float img[3][227][227]={0.0};\n",
        "\n",
        "\tvector< vector<vector< vector <float> > > > \n",
        "\t\tconv1(64,vector<vector< vector <float> > >(3,vector< vector <float> >(11,vector <float>(11,0.0)))),\n",
        "\t\tconv2(192,vector<vector< vector <float> > >(64,vector< vector <float> >(5,vector <float>(5,0.0)))),\n",
        "\t\tconv3(384,vector<vector< vector <float> > >(192,vector< vector <float> >(3,vector <float>(3,0.0)))),\n",
        "\t\tconv4(256,vector<vector< vector <float> > >(384,vector< vector <float> >(3,vector <float>(3,0.0)))),\n",
        "\t\tconv5(256,vector<vector< vector <float> > >(256,vector< vector <float> >(3,vector <float>(3,0.0))))\n",
        "\t;\n",
        "\n",
        "\tvector< vector <float> > \n",
        "\t\tlin6(4096,  vector <float> (12544, 0.0)),\n",
        "\t\tlin7(4096,  vector <float> (4096, 0.0)),\n",
        "\t\tlin8(1000,  vector <float> (4096, 0.0));\n",
        "\t//9216 appended with zeroes from 36 to 49\n",
        "\tfloat bias1[64]={0.0}, bias2[192]={0.0}, bias3[384]={0.0}, bias4[256]={0.0}, bias5[256]={0.0},bias6[4096]={0.0}, bias7[4096]={0.0}, bias8[1000]={0.0};\n",
        "\n",
        "\tfill_vec(conv1, bias1, \"/content/drive/MyDrive/HP3/feature1.txt\", 64, 3, 11, 11);\n",
        "\tfill_vec(conv2, bias2, \"/content/drive/MyDrive/HP3/feature2.txt\", 192, 64, 5, 5);\n",
        "\tfill_vec(conv3, bias3, \"/content/drive/MyDrive/HP3/feature3.txt\", 384, 192, 3, 3);\n",
        "\tfill_vec(conv4, bias4, \"/content/drive/MyDrive/HP3/feature4.txt\", 256, 384, 3, 3);\n",
        "\tfill_vec(conv5, bias5, \"/content/drive/MyDrive/HP3/feature5.txt\", 256, 256, 3, 3);\n",
        "\n",
        "\tfill_vec_special(lin6, bias6, \"/content/drive/MyDrive/HP3/feature7.txt\", 4096, 12544);\n",
        "\n",
        "\tfill_vec(lin7, bias7, \"/content/drive/MyDrive/HP3/feature7.txt\", 4096, 4096);\n",
        "\tfill_vec(lin8, bias8, \"/content/drive/MyDrive/HP3/feature8.txt\", 1000, 4096);\n",
        "\n",
        "    poolDim_t poolDim1 = setPoolSpecs((bool)OVERLAP_POOLING);\n",
        "\tconvDim_t firstLayerInputDims = setConvSpecs(227, 227, 3, BATCH_SIZE);\n",
        "\tkernelDim_t layerKernel1 = setKernelSpecs(64,11,11,4,4,2,2,1,1);\n",
        "    convlayer1.set(1, firstLayerInputDims, layerKernel1, poolDim1, alpha, beta, CUDNN_TENSOR_NHWC, CUDNN_DATA_FLOAT, CUDNN_CROSS_CORRELATION,\n",
        "\t\t\t\t\t\tCUDNN_ACTIVATION_RELU, CUDNN_POOLING_MAX);\n",
        "\tconvlayer1.getConvLayerSpecs();\n",
        "\tconvlayer1.buildConvLayer(bias1, conv1);\n",
        "\n",
        "\n",
        "\n",
        "\tpoolDim_t poolDim2 = setPoolSpecs((bool)OVERLAP_POOLING);\n",
        "\tkernelDim_t layerKernel2 = setKernelSpecs(192,5,5,1,1,2,2,1,1);\n",
        "\tconvlayer2.set(2, convlayer1.outDims, layerKernel2, poolDim2, alpha, beta, CUDNN_TENSOR_NHWC, CUDNN_DATA_FLOAT, CUDNN_CROSS_CORRELATION,\n",
        "\t\t\t\t\t\tCUDNN_ACTIVATION_RELU, CUDNN_POOLING_MAX);\n",
        "\tconvlayer2.getConvLayerSpecs();\n",
        "\tconvlayer2.buildConvLayer(bias2, conv2);\n",
        "\n",
        "\t// poolDim_t poolDim2 = setPoolSpecs((bool)OVERLAP_POOLING);\n",
        "\tkernelDim_t layerKernel3 = setKernelSpecs(384,3,3,1,1,1,1,1,1);\n",
        "\tconvlayer3.set(3, convlayer2.outDims, layerKernel3, alpha, beta, CUDNN_TENSOR_NHWC, CUDNN_DATA_FLOAT, CUDNN_CROSS_CORRELATION,\n",
        "\t\t\t\t\t\tCUDNN_ACTIVATION_RELU);\n",
        "\tconvlayer3.getConvLayerSpecs();\n",
        "\tconvlayer3.buildConvLayer(bias3, conv3);\n",
        "\n",
        "\n",
        "\t// poolDim_t poolDim2 = setPoolSpecs((bool)OVERLAP_POOLING);\n",
        "\tkernelDim_t layerKernel4 = setKernelSpecs(256,3,3,1,1,1,1,1,1);\n",
        "\tconvlayer4.set(4, convlayer3.outDims, layerKernel4, alpha, beta, CUDNN_TENSOR_NHWC, CUDNN_DATA_FLOAT, CUDNN_CROSS_CORRELATION,\n",
        "\t\t\t\t\t\tCUDNN_ACTIVATION_RELU);\n",
        "\tconvlayer4.getConvLayerSpecs();\n",
        "\tconvlayer4.buildConvLayer(bias4, conv4);\n",
        "\n",
        "\n",
        "\tpoolDim_t poolDim5 = setPoolSpecs((bool)OVERLAP_POOLING);\n",
        "\tkernelDim_t layerKernel5 = setKernelSpecs(256,3,3,1,1,1,1,1,1);\n",
        "\tconvlayer5.set(5, convlayer4.outDims, layerKernel5, poolDim5, alpha, beta, CUDNN_TENSOR_NHWC, CUDNN_DATA_FLOAT, CUDNN_CROSS_CORRELATION,\n",
        "\t\t\t\t\t\tCUDNN_ACTIVATION_RELU, CUDNN_POOLING_MAX);\n",
        "\tconvlayer5.getConvLayerSpecs();\n",
        "\tconvlayer5.buildConvLayer(bias5, conv5); \n",
        "\n",
        "    fclayer1.set(7*7*256, BATCH_SIZE, 4096 , alpha, beta, CUDNN_ACTIVATION_RELU,CUDNN_TENSOR_NHWC,\n",
        "          CUDNN_DATA_FLOAT);\n",
        "    fclayer1.getFCLayerSpecs();\n",
        "    fclayer1.buildFCLayer(bias6, lin6);\n",
        "\n",
        "\n",
        "    fclayer2.set(fclayer1.outDims, BATCH_SIZE, 4096 , alpha, beta, CUDNN_ACTIVATION_RELU,CUDNN_TENSOR_NHWC,\n",
        "            CUDNN_DATA_FLOAT);\n",
        "    fclayer2.getFCLayerSpecs();\n",
        "    fclayer2.buildFCLayer(bias7, lin7);\n",
        "\n",
        "\n",
        "    fclayer3.set(fclayer2.outDims, BATCH_SIZE, 1000 , alpha, beta, CUDNN_ACTIVATION_RELU,CUDNN_TENSOR_NHWC,\n",
        "          CUDNN_DATA_FLOAT);\n",
        "    fclayer3.getFCLayerSpecs();\n",
        "    fclayer3.buildFCLayer(bias8, lin8);\n",
        "\tcout<<\"BUILT ALL LAYERS\"<<endl;\n",
        "\n",
        "    cudaStream_t stream1, stream2;\n",
        "    cudaStreamCreate(&stream1);\n",
        "    cudaStreamCreate(&stream2);\n",
        "\n",
        "    cudnnHandle_t cudnn1, cudnn2;\n",
        "\tcheckCUDNN(cudnnCreate(&cudnn1));\n",
        "    cudnnSetStream(cudnn1, stream1);\n",
        "\n",
        "    checkCUDNN(cudnnCreate(&cudnn2));\n",
        "    cudnnSetStream(cudnn2, stream2);\n",
        "\n",
        "\tcublasHandle_t cublas1, cublas2;\n",
        "\tcublasCreate(&cublas1);\n",
        "    cublasCreate(&cublas2);\n",
        "\n",
        "    cublasSetStream(cublas1, stream1);\n",
        "    cublasSetStream(cublas2, stream2);\n",
        "\n",
        "\n",
        "    SET pq;\n",
        "    Kernel k1(stream1,\n",
        "        cublas1,\n",
        "        cudnn1,\n",
        "        1,\n",
        "        img1,\n",
        "        0,\n",
        "        50,\n",
        "        0.6,\n",
        "        1);\n",
        "\n",
        "    cout<<\"K1:\"<<k1.tbRatio<<\" \"<<k1.compTime<<\" \"<<k1.execTime<<endl;\n",
        "\n",
        "    Kernel k2(stream1,\n",
        "        cublas1,\n",
        "        cudnn1,\n",
        "        1,\n",
        "        img1,\n",
        "        0,\n",
        "        30,\n",
        "        0.6,\n",
        "        2);\n",
        "\n",
        "    Kernel k3(stream1,\n",
        "        cublas1,\n",
        "        cudnn1,\n",
        "        1,\n",
        "        img1,\n",
        "        0,\n",
        "        40,\n",
        "        0.3,\n",
        "        3);\n",
        "    \n",
        "    for(int i=0;i<100;i++){\n",
        "        Kernel k(stream1,\n",
        "            cublas1,\n",
        "            cudnn1,\n",
        "            1,\n",
        "            img1,\n",
        "            0,\n",
        "            50,\n",
        "            0.6,\n",
        "            i);\n",
        "\n",
        "        pq = enqueue(pq, k);\n",
        "    }\n",
        "\n",
        "    while(!pq.empty()){\n",
        "        pq = dequeue(pq);\n",
        "    }\n",
        "\n",
        "\n",
        "    //cublasSetStream(cublas2, stream2);\n",
        "\n",
        "    // float ****output11, ****output12, ****output13, ****output14, ****output15, **output16,**output17, **output18;\n",
        "    // //float ****output21, ****output22, ****output23, ****output24, ****output25, **output26,**output27, **output28;\n",
        "\n",
        "    // call_conv1(convlayer1, stream1, cudnn1, img1, output11);\n",
        "    // call_conv2(convlayer2, stream1, cudnn1, output11, output12);\n",
        "    // call_conv3(convlayer3, stream1, cudnn1, output12, output13);\n",
        "    // call_conv4(convlayer4, stream1, cudnn1, output13, output14);\n",
        "    // call_conv5(convlayer5, stream1, cudnn1, output14, output15);\n",
        "    // cudaStreamSynchronize(stream1);\n",
        "    // FreeMem(output11, 1, 64, 28, 28);\n",
        "    // FreeMem(output12, 1, 192, 14, 14);\n",
        "    // FreeMem(output13, 1, 384, 14, 14);\n",
        "    // FreeMem(output14, 1, 256, 14, 14);\n",
        "\n",
        "    // call_fc1(fclayer1, stream1, cudnn1,  cublas1, output15, output16);\n",
        "    // //call_fc2(fclayer2, stream1, cudnn1,  cublas1, output16, output17);\n",
        "    // cudaStreamSynchronize(stream1);\n",
        "\n",
        "    // call_fc3(fclayer3, stream1, cudnn1,  cublas1, output16, output18);\n",
        "    // cudaStreamSynchronize(stream1);\n",
        "    /**\n",
        "    call_conv1(convlayer1, stream1, cudnn1, img2, output21);\n",
        "    call_conv2(convlayer2, stream1, cudnn1, output21, output22);\n",
        "    call_conv3(convlayer3, stream1, cudnn1, output22, output23);\n",
        "    call_conv4(convlayer4, stream1, cudnn1, output23, output24);\n",
        "    call_conv5(convlayer5, stream1, cudnn1, output24, output25);\n",
        "    call_fc1(fclayer1, stream1, cudnn1,  cublas1, output25, output26);\n",
        "    call_fc2(fclayer2, stream1, cudnn1,  cublas1, output26, output27);\n",
        "    call_fc3(fclayer3, stream1, cudnn1,  cublas1, output27, output28);\n",
        "    **/\n",
        "    // cout<<output18[0][0]<<endl;\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/alex.cu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzBcp-gHn1ms",
        "outputId": "7e6a4f6e-f12e-45d1-e8a6-3d9d78c21501"
      },
      "source": [
        "!nvcc /content/src/alex.cu -o /content/src/hellocuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tmp/tmpxft_00000298_00000000-11_alex.o: In function `ConvLayers::getConvLayerSpecs()':\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x45b): undefined reference to `cudnnCreateTensorDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x4a0): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x50e): undefined reference to `cudnnSetTensor4dDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x557): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x593): undefined reference to `cudnnCreateFilterDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x5d8): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x646): undefined reference to `cudnnSetFilter4dDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x68f): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x6cb): undefined reference to `cudnnCreateConvolutionDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x710): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x798): undefined reference to `cudnnSetConvolution2dDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x7e1): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x865): undefined reference to `cudnnGetConvolution2dForwardOutputDim'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x8ae): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x92e): undefined reference to `cudnnCreateTensorDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x973): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x9ed): undefined reference to `cudnnSetTensor4dDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0xa36): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0xa72): undefined reference to `cudnnCreateTensorDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0xab7): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0xb31): undefined reference to `cudnnSetTensor4dDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0xb7a): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0xbb6): undefined reference to `cudnnCreateActivationDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0xbfb): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0xc4a): undefined reference to `cudnnSetActivationDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0xc8f): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0xd4e): undefined reference to `cudnnCreatePoolingDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0xd93): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0xe1f): undefined reference to `cudnnSetPooling2dDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0xe68): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0xee2): undefined reference to `cudnnGetPooling2dForwardOutputDim'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0xf27): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0xf63): undefined reference to `cudnnCreateTensorDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0xfa8): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x1022): undefined reference to `cudnnSetTensor4dDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x106b): undefined reference to `cudnnGetErrorString'\n",
            "/tmp/tmpxft_00000298_00000000-11_alex.o: In function `ConvLayers::fwdProp(CUstream_st*, cudnnContext*, float*, float*&, float*&, float*&, void*&)':\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x18c2): undefined reference to `cudnnGetConvolutionForwardAlgorithm'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x190b): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x1989): undefined reference to `cudnnGetConvolutionForwardWorkspaceSize'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x19d2): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x1b8d): undefined reference to `cudnnConvolutionForward'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x1bd6): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x1c4f): undefined reference to `cudnnAddTensor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x1c98): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x1d15): undefined reference to `cudnnActivationForward'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x1d5e): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x1df1): undefined reference to `cudnnPoolingForward'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x1e3a): undefined reference to `cudnnGetErrorString'\n",
            "/tmp/tmpxft_00000298_00000000-11_alex.o: In function `FCLayers::getFCLayerSpecs()':\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x1f25): undefined reference to `cudnnCreateActivationDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x1f6a): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x1fb9): undefined reference to `cudnnSetActivationDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x1ffe): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x203a): undefined reference to `cudnnCreateTensorDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x207f): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x20ed): undefined reference to `cudnnSetTensor4dDescriptor'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x2136): undefined reference to `cudnnGetErrorString'\n",
            "/tmp/tmpxft_00000298_00000000-11_alex.o: In function `FCLayers::fwdProp(cublasContext*, cudnnContext*, CUstream_st*, float*, float*&)':\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x24f9): undefined reference to `cublasSgemm_v2'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x25eb): undefined reference to `cudnnActivationForward'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x2634): undefined reference to `cudnnGetErrorString'\n",
            "/tmp/tmpxft_00000298_00000000-11_alex.o: In function `main':\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x6766): undefined reference to `cudnnCreate'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x67b4): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x67f7): undefined reference to `cudnnSetStream'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x6806): undefined reference to `cudnnCreate'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x6854): undefined reference to `cudnnGetErrorString'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x6897): undefined reference to `cudnnSetStream'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x68a6): undefined reference to `cublasCreate_v2'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x68b5): undefined reference to `cublasCreate_v2'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x68ce): undefined reference to `cublasSetStream_v2'\n",
            "tmpxft_00000298_00000000-6_alex.cudafe1.cpp:(.text+0x68e7): undefined reference to `cublasSetStream_v2'\n",
            "collect2: error: ld returned 1 exit status\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGmjL2fcoHGm",
        "outputId": "2304ab0f-3198-4df5-8e1c-8c488e7218ba"
      },
      "source": [
        "!/content/src/hellocuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: /content/src/hellocuda: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rE0LAgiy1Kiw",
        "outputId": "b1391884-f55b-459a-9cd7-3d579a605bb5"
      },
      "source": [
        "%%cuda --name hello.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void vectorAdd(const float *A, const float *B, float *C, int numElements)\n",
        "{\n",
        "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "\n",
        "    if (i < numElements)\n",
        "    {\n",
        "        C[i] = A[i] + B[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "    // Error code to check return values for CUDA calls\n",
        "    cudaError_t err = cudaSuccess;\n",
        "\n",
        "    // Print the vector length to be used, and compute its size\n",
        "    int numElements = 0;\n",
        "    scanf(\"%d\",&numElements);\n",
        "    \n",
        "    size_t size = numElements * sizeof(float);\n",
        "    printf(\"[Vector addition of %d elements]\\n\", numElements);\n",
        "\n",
        "    // Allocate the host input vector A\n",
        "    float *h_A = (float *)malloc(size);\n",
        "\n",
        "    // Allocate the host input vector B\n",
        "    float *h_B = (float *)malloc(size);\n",
        "\n",
        "    // Allocate the host output vector C\n",
        "    float *h_C = (float *)malloc(size);\n",
        "\n",
        "    // Verify that allocations succeeded\n",
        "    if (h_A == NULL || h_B == NULL || h_C == NULL)\n",
        "    {\n",
        "        fprintf(stderr, \"Failed to allocate host vectors!\\n\");\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    // Initialize the host input vectors\n",
        "    for (int i = 0; i < numElements; ++i)\n",
        "    {\n",
        "        h_A[i] = rand()/(float)RAND_MAX;\n",
        "        h_B[i] = rand()/(float)RAND_MAX;\n",
        "    }\n",
        "\n",
        "    // Allocate the device input vector A\n",
        "    float *d_A = NULL;\n",
        "    err = cudaMalloc((void **)&d_A, size);\n",
        "\n",
        "    if (err != cudaSuccess)\n",
        "    {\n",
        "        fprintf(stderr, \"Failed to allocate device vector A (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    // Allocate the device input vector B\n",
        "    float *d_B = NULL;\n",
        "    err = cudaMalloc((void **)&d_B, size);\n",
        "\n",
        "    if (err != cudaSuccess)\n",
        "    {\n",
        "        fprintf(stderr, \"Failed to allocate device vector B (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    // Allocate the device output vector C\n",
        "    float *d_C = NULL;\n",
        "    err = cudaMalloc((void **)&d_C, size);\n",
        "\n",
        "    if (err != cudaSuccess)\n",
        "    {\n",
        "        fprintf(stderr, \"Failed to allocate device vector C (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    // Copy the host input vectors A and B in host memory to the device input vectors in\n",
        "    // device memory\n",
        "    printf(\"Copy input data from the host memory to the CUDA device\\n\");\n",
        "    err = cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    if (err != cudaSuccess)\n",
        "    {\n",
        "        fprintf(stderr, \"Failed to copy vector A from host to device (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    err = cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    if (err != cudaSuccess)\n",
        "    {\n",
        "        fprintf(stderr, \"Failed to copy vector B from host to device (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    // Launch the Vector Add CUDA Kernel\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid =(numElements + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    printf(\"CUDA kernel launch with %d blocks of %d threads\\n\", blocksPerGrid, threadsPerBlock);\n",
        "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, numElements);\n",
        "    err = cudaGetLastError();\n",
        "\n",
        "    if (err != cudaSuccess)\n",
        "    {\n",
        "        fprintf(stderr, \"Failed to launch vectorAdd kernel (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    // Copy the device result vector in device memory to the host result vector\n",
        "    // in host memory.\n",
        "    printf(\"Copy output data from the CUDA device to the host memory\\n\");\n",
        "    err = cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    if (err != cudaSuccess)\n",
        "    {\n",
        "        fprintf(stderr, \"Failed to copy vector C from device to host (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    // Verify that the result vector is correct\n",
        "    for (int i = 0; i < numElements; ++i)\n",
        "    {\n",
        "        if (fabs(h_A[i] + h_B[i] - h_C[i]) > 1e-5)\n",
        "        {\n",
        "            fprintf(stderr, \"Result verification failed at element %d!\\n\", i);\n",
        "            exit(EXIT_FAILURE);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    printf(\"Test PASSED\\n\");\n",
        "\n",
        "    // Free device global memory\n",
        "    err = cudaFree(d_A);\n",
        "\n",
        "    if (err != cudaSuccess)\n",
        "    {\n",
        "        fprintf(stderr, \"Failed to free device vector A (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    err = cudaFree(d_B);\n",
        "\n",
        "    if (err != cudaSuccess)\n",
        "    {\n",
        "        fprintf(stderr, \"Failed to free device vector B (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    err = cudaFree(d_C);\n",
        "\n",
        "    if (err != cudaSuccess)\n",
        "    {\n",
        "        fprintf(stderr, \"Failed to free device vector C (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    // Free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "\n",
        "    // Reset the device and exit\n",
        "    // cudaDeviceReset causes the driver to clean up all state. While\n",
        "    // not mandatory in normal operation, it is good practice.  It is also\n",
        "    // needed to ensure correct operation when the application is being\n",
        "    // profiled. Calling cudaDeviceReset causes all profile data to be\n",
        "    // flushed before the application exits\n",
        "    err = cudaDeviceReset();\n",
        "\n",
        "    if (err != cudaSuccess)\n",
        "    {\n",
        "        fprintf(stderr, \"Failed to deinitialize the device! error=%s\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    printf(\"Done\\n\");\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/hello.cu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAyjsgWL5JGH"
      },
      "source": [
        "!nvcc src/hello.cu -o src/hello"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT6n4zFs5Nnk",
        "outputId": "2e4a1b57-5fa8-4c1d-9104-14f84a07e070"
      },
      "source": [
        "!src/hello"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "[Vector addition of 100 elements]\n",
            "Copy input data from the host memory to the CUDA device\n",
            "CUDA kernel launch with 1 blocks of 256 threads\n",
            "Copy output data from the CUDA device to the host memory\n",
            "Test PASSED\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB8t6bGp5Q20"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
